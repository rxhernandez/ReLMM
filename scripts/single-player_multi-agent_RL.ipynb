{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dd3933a-cd58-4074-9fa9-f9a9ec18b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# User defined files and classes\n",
    "import sys\n",
    "from read_data import inputs\n",
    "import utils_dataset as utilsd\n",
    "from environment import Environment\n",
    "from qlearning import QNetwork\n",
    "from predictor_models import predictor_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed81dc-10bc-475f-96a5-a8de8435eaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3771a5d8-ccfd-41cd-9983-ad90a1e0aaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data for the input dataset type:  Gryffin\n",
      "This class contains the predictor models to generate rewards for agents\n",
      "Episode: 1, state: [1 1 1 0 0 1 1 1 0 1 1 1 0 0], Total Rewards: 1042.390102724037, Epsilon: 0.995\n",
      "Episode: 2, state: [0 0 1 1 0 1 1 1 1 1 1 0 1 0], Total Rewards: 988.8771750716556, Epsilon: 0.990025\n",
      "Episode: 3, state: [0 0 1 0 0 1 0 1 0 1 0 0 1 0], Total Rewards: 979.5143832397449, Epsilon: 0.985074875\n",
      "Episode: 4, state: [1 0 0 0 1 0 0 1 0 1 0 1 1 0], Total Rewards: 1001.584056709807, Epsilon: 0.9801495006250001\n",
      "Episode: 5, state: [0 1 0 1 0 1 1 0 0 1 1 1 1 1], Total Rewards: 1045.6921844563403, Epsilon: 0.9752487531218751\n",
      "Episode: 6, state: [1 0 1 0 1 1 1 0 1 0 1 0 0 0], Total Rewards: 1077.9913839361263, Epsilon: 0.9703725093562657\n",
      "Episode: 7, state: [0 0 0 0 0 1 0 0 1 1 1 0 0 1], Total Rewards: 1001.7043400345137, Epsilon: 0.9655206468094844\n",
      "Episode: 8, state: [0 0 0 0 1 0 1 1 1 0 1 1 1 0], Total Rewards: 1053.3186913444574, Epsilon: 0.960693043575437\n",
      "Episode: 9, state: [0 0 0 1 0 1 0 0 1 1 0 0 1 0], Total Rewards: 1081.6964085201726, Epsilon: 0.9558895783575597\n",
      "Episode: 10, state: [0 0 1 1 1 1 0 0 0 0 1 0 0 1], Total Rewards: 1015.7971873942921, Epsilon: 0.9511101304657719\n",
      "Episode: 11, state: [1 0 1 0 0 0 1 0 0 0 1 1 1 0], Total Rewards: 1033.553117554931, Epsilon: 0.946354579813443\n",
      "Episode: 12, state: [0 1 1 1 0 1 1 0 0 1 1 0 0 0], Total Rewards: 1026.106709698085, Epsilon: 0.9416228069143757\n",
      "Episode: 13, state: [1 1 0 0 1 0 0 1 0 0 0 1 1 0], Total Rewards: 951.7201775303411, Epsilon: 0.9369146928798039\n",
      "Episode: 14, state: [0 0 0 1 0 0 0 1 1 1 1 0 0 1], Total Rewards: 1062.088622158888, Epsilon: 0.9322301194154049\n",
      "Episode: 15, state: [1 1 1 0 1 1 1 0 0 0 0 0 0 1], Total Rewards: 987.2378925003193, Epsilon: 0.9275689688183278\n",
      "Episode: 16, state: [0 1 0 0 0 0 0 1 1 1 0 1 0 0], Total Rewards: 1053.8565985043444, Epsilon: 0.9229311239742362\n",
      "Episode: 17, state: [1 0 0 0 1 1 1 1 0 0 1 0 1 1], Total Rewards: 1044.5536385618886, Epsilon: 0.918316468354365\n",
      "Episode: 18, state: [1 0 1 1 0 1 0 0 1 0 1 0 1 1], Total Rewards: 1022.3874234105295, Epsilon: 0.9137248860125932\n",
      "Episode: 19, state: [1 0 0 0 0 0 1 1 0 1 0 0 0 0], Total Rewards: 959.3125998095636, Epsilon: 0.9091562615825302\n",
      "Episode: 20, state: [1 1 1 1 1 0 0 1 1 0 0 0 0 1], Total Rewards: 1012.8164612074553, Epsilon: 0.9046104802746175\n",
      "Episode: 21, state: [0 1 1 1 0 1 1 1 0 1 1 1 0 1], Total Rewards: 1008.9036682187517, Epsilon: 0.9000874278732445\n",
      "Episode: 22, state: [1 1 0 0 1 1 0 0 0 0 0 1 0 1], Total Rewards: 955.8858533036489, Epsilon: 0.8955869907338783\n",
      "Episode: 23, state: [1 1 0 0 0 1 1 0 1 1 0 1 0 1], Total Rewards: 965.2498654837557, Epsilon: 0.8911090557802088\n",
      "Episode: 24, state: [0 1 1 0 1 0 1 1 1 1 0 0 1 1], Total Rewards: 985.7695392078083, Epsilon: 0.8866535105013078\n",
      "Episode: 25, state: [0 0 1 0 0 0 1 0 0 1 1 0 0 1], Total Rewards: 1027.6949861667165, Epsilon: 0.8822202429488013\n",
      "Episode: 26, state: [1 0 1 1 0 0 0 1 1 0 0 1 0 1], Total Rewards: 1082.0008038930894, Epsilon: 0.8778091417340573\n",
      "Episode: 27, state: [0 1 0 1 1 1 0 1 0 0 1 0 0 0], Total Rewards: 1099.3574005267092, Epsilon: 0.8734200960253871\n",
      "Episode: 28, state: [0 1 0 1 0 0 1 1 1 0 1 0 1 1], Total Rewards: 1034.369140932779, Epsilon: 0.8690529955452602\n",
      "Episode: 29, state: [0 1 0 1 1 0 0 1 1 0 1 0 1 1], Total Rewards: 1025.2571195097469, Epsilon: 0.8647077305675338\n",
      "Episode: 30, state: [0 0 0 1 1 1 0 0 0 1 0 0 1 1], Total Rewards: 1030.7010239591334, Epsilon: 0.8603841919146962\n",
      "Episode: 31, state: [1 1 1 1 1 1 1 1 0 1 0 1 1 1], Total Rewards: 1026.5064744170822, Epsilon: 0.8560822709551227\n",
      "Episode: 32, state: [0 0 1 1 0 0 1 1 1 1 0 1 0 1], Total Rewards: 1061.5983848041449, Epsilon: 0.851801859600347\n",
      "Episode: 33, state: [0 1 0 1 0 0 0 1 0 0 1 1 1 0], Total Rewards: 1052.303351317515, Epsilon: 0.8475428503023453\n",
      "Episode: 34, state: [1 0 1 0 1 1 1 1 0 0 1 0 0 0], Total Rewards: 1004.5713615766144, Epsilon: 0.8433051360508336\n",
      "Episode: 35, state: [0 0 1 0 0 1 0 0 0 1 0 1 1 0], Total Rewards: 979.5253155928505, Epsilon: 0.8390886103705794\n",
      "Episode: 36, state: [0 1 1 0 0 0 1 1 1 0 0 0 1 1], Total Rewards: 1054.9421671953028, Epsilon: 0.8348931673187264\n",
      "Episode: 37, state: [1 0 0 0 0 0 0 0 1 0 1 1 1 1], Total Rewards: 987.1859878303244, Epsilon: 0.8307187014821328\n",
      "Episode: 38, state: [0 0 0 0 0 1 0 0 1 1 1 1 1 1], Total Rewards: 1035.034726367599, Epsilon: 0.8265651079747222\n",
      "Episode: 39, state: [0 0 0 0 0 0 0 0 1 0 1 1 0 0], Total Rewards: 990.3678845280763, Epsilon: 0.8224322824348486\n",
      "Episode: 40, state: [1 0 1 0 0 0 1 1 0 0 0 1 0 1], Total Rewards: 960.118874401957, Epsilon: 0.8183201210226743\n",
      "Episode: 41, state: [1 0 1 0 0 0 1 1 0 1 1 1 1 1], Total Rewards: 1010.1703009872472, Epsilon: 0.8142285204175609\n",
      "Episode: 42, state: [0 1 1 0 1 1 1 0 1 0 1 0 0 0], Total Rewards: 1015.1074320125923, Epsilon: 0.810157377815473\n",
      "Episode: 43, state: [0 1 0 0 0 1 0 1 0 0 1 0 0 1], Total Rewards: 984.6451952201231, Epsilon: 0.8061065909263957\n",
      "Episode: 44, state: [1 1 0 1 0 1 1 1 1 1 0 1 0 1], Total Rewards: 1075.9543970139948, Epsilon: 0.8020760579717637\n",
      "Episode: 45, state: [0 1 0 1 0 1 0 0 1 0 1 1 0 1], Total Rewards: 1063.8103523172852, Epsilon: 0.798065677681905\n",
      "Episode: 46, state: [1 1 1 1 1 1 1 1 1 1 1 0 1 1], Total Rewards: 1027.581457728572, Epsilon: 0.7940753492934954\n",
      "Episode: 47, state: [0 0 1 0 0 0 1 0 0 1 0 0 0 0], Total Rewards: 978.4942131189033, Epsilon: 0.7901049725470279\n",
      "Episode: 48, state: [0 0 1 0 0 1 1 1 0 1 0 0 0 0], Total Rewards: 1079.8248320403159, Epsilon: 0.7861544476842928\n",
      "Episode: 49, state: [1 1 0 0 0 1 1 1 1 1 1 0 1 1], Total Rewards: 1090.9835070219374, Epsilon: 0.7822236754458713\n",
      "Episode: 50, state: [0 1 0 0 1 1 0 1 0 1 0 0 0 0], Total Rewards: 1021.7632344559798, Epsilon: 0.778312557068642\n",
      "Episode: 51, state: [1 0 0 1 0 0 1 0 0 0 0 0 1 1], Total Rewards: 1028.9018638477348, Epsilon: 0.7744209942832988\n",
      "Episode: 52, state: [1 1 0 1 1 0 1 1 0 1 1 1 0 1], Total Rewards: 998.3595304482358, Epsilon: 0.7705488893118823\n",
      "Episode: 53, state: [1 1 0 1 1 1 1 1 0 1 1 1 0 0], Total Rewards: 1068.356847617905, Epsilon: 0.7666961448653229\n",
      "Episode: 54, state: [0 0 1 0 1 0 1 1 0 0 0 1 1 1], Total Rewards: 1028.564025227272, Epsilon: 0.7628626641409962\n",
      "Episode: 55, state: [0 0 0 1 0 0 0 1 1 1 0 1 0 1], Total Rewards: 981.6066293688104, Epsilon: 0.7590483508202912\n",
      "Episode: 56, state: [1 0 1 0 0 0 1 1 1 0 0 1 1 1], Total Rewards: 1076.906257010347, Epsilon: 0.7552531090661897\n",
      "Episode: 57, state: [1 0 0 1 0 1 1 0 1 1 1 0 0 1], Total Rewards: 1028.4707916437424, Epsilon: 0.7514768435208588\n",
      "Episode: 58, state: [1 1 0 1 1 1 1 1 1 1 0 1 1 0], Total Rewards: 1010.0518411304149, Epsilon: 0.7477194593032545\n",
      "Episode: 59, state: [0 0 0 1 1 1 0 0 1 1 1 0 1 0], Total Rewards: 1088.6494798017022, Epsilon: 0.7439808620067382\n",
      "Episode: 60, state: [1 1 1 1 1 1 1 0 1 1 1 0 1 0], Total Rewards: 1082.6066355160388, Epsilon: 0.7402609576967045\n",
      "Episode: 61, state: [1 1 0 0 1 0 0 1 0 0 0 0 0 1], Total Rewards: 998.2872355658714, Epsilon: 0.736559652908221\n",
      "Episode: 62, state: [0 1 1 0 0 0 0 0 1 0 0 1 0 1], Total Rewards: 991.7992763782361, Epsilon: 0.7328768546436799\n",
      "Episode: 63, state: [1 1 1 0 0 0 0 1 1 0 0 0 1 0], Total Rewards: 1012.3341292376659, Epsilon: 0.7292124703704616\n",
      "Episode: 64, state: [0 1 1 1 0 1 0 1 0 1 1 0 1 0], Total Rewards: 1045.362017513384, Epsilon: 0.7255664080186093\n",
      "Episode: 65, state: [0 0 0 1 1 0 1 1 0 1 0 1 0 0], Total Rewards: 1020.7999228892699, Epsilon: 0.7219385759785162\n",
      "Episode: 66, state: [0 0 1 1 1 0 0 0 1 0 1 1 1 1], Total Rewards: 1017.1102786575952, Epsilon: 0.7183288830986236\n",
      "Episode: 67, state: [0 1 1 1 1 1 1 1 1 0 0 1 0 1], Total Rewards: 1003.6214225554979, Epsilon: 0.7147372386831305\n",
      "Episode: 68, state: [1 0 1 1 0 1 1 1 0 0 0 0 1 1], Total Rewards: 1021.9146192962372, Epsilon: 0.7111635524897149\n",
      "Episode: 69, state: [0 0 0 1 1 1 0 0 0 1 0 0 1 1], Total Rewards: 1111.2111905388024, Epsilon: 0.7076077347272662\n",
      "Episode: 70, state: [0 0 0 1 1 0 0 1 0 1 0 1 0 1], Total Rewards: 1035.8491817510076, Epsilon: 0.7040696960536299\n",
      "Episode: 71, state: [1 0 0 0 0 0 1 1 1 0 1 0 1 0], Total Rewards: 965.6394499341982, Epsilon: 0.7005493475733617\n",
      "Episode: 72, state: [0 0 0 0 1 1 1 0 0 1 0 0 1 1], Total Rewards: 1085.5529211858407, Epsilon: 0.697046600835495\n",
      "Episode: 73, state: [1 1 1 0 0 0 1 1 0 1 0 1 0 1], Total Rewards: 1037.4579352160458, Epsilon: 0.6935613678313175\n"
     ]
    }
   ],
   "source": [
    "## Main Function\n",
    "\n",
    "# Read input dataset\n",
    "run_folder = '/Users/maitreyeesharma/WORKSPACE/PostDoc/Chemistry/SPIRAL/codes/RL/ReLMM/scripts/'\n",
    "\n",
    "# Reading the input json file with dataset filename and path information\n",
    "with open(run_folder+'inputs.json', \"r\") as f:\n",
    "    input_dict = json.load(f)\n",
    "\n",
    "input_type = input_dict['InputType']\n",
    "input_path = input_dict['InputPath']\n",
    "input_file = input_dict['InputFile']\n",
    "output_dir = input_dict['OutputDirectory']\n",
    "\n",
    "# Create a new output directory if it does not exist\n",
    "isExist = os.path.exists(output_dir)\n",
    "if not isExist:\n",
    "    os.makedirs(output_dir)\n",
    "    print(\"The new directory is created!\", output_dir)\n",
    "\n",
    "input_data = inputs(input_type=input_type,\n",
    "                           input_path=input_path,\n",
    "                           input_file=input_file)\n",
    "\n",
    "X_data, Y_data, descriptors = input_data.read_inputs()\n",
    "X_stand_all, X_stand_df_all, scalerX = utilsd.standardize_data(X_data)\n",
    "Y_stand_all, Y_stand_df_all, scalerY = utilsd.standardize_data(pd.DataFrame({'target':Y_data[:,0]}))\n",
    "# X_stand_df, X_test_df, Y_stand_df, Y_test_df = train_test_split(X_stand_df_all, Y_stand_df_all, test_size=0.1, random_state=40)\n",
    "# X_stand, X_test, Y_stand, Y_test = train_test_split(X_stand_all, Y_stand_all, test_size=0.1, random_state=40)\n",
    "X_stand_df = X_stand_df_all\n",
    "Y_stand_df = Y_stand_df_all\n",
    "\n",
    "# Dataset parameters\n",
    "total_num_features = len(descriptors)\n",
    "\n",
    "# Environment parameters\n",
    "state_size = total_num_features  # Size of the state space\n",
    "N_agents = total_num_features # Number of agents\n",
    "action_size = 2  # Number of possible actions\n",
    "N_steps = 100 # Number of steps to take per episode\n",
    "predictor_model = predictor_models()\n",
    "\n",
    "# Hyperparameters\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_decay = 0.995  # Decay rate of exploration\n",
    "gamma = 0.95  # Discount factor\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize environment and Q-networks for each agent\n",
    "env = Environment(state_size,action_size,N_agents,N_steps)\n",
    "agent_model = {}\n",
    "agent_optimizer = {}\n",
    "agent_qvalue = {}\n",
    "\n",
    "for i_agent in range(N_agents):\n",
    "    model_name = 'agent'+str(i_agent)+'_model'\n",
    "    optimizer_name = 'agent'+str(i_agent)+'_optimizer'\n",
    "    agent_model[model_name] = QNetwork(env.state_size, env.action_size)\n",
    "    agent_optimizer[optimizer_name] = optim.Adam(agent_model[model_name].parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_episodes = 1000\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    total_rewards = np.zeros(N_agents) # Total rewards for agents\n",
    "    \n",
    "    while True:\n",
    "        # Agents choose actions using epsilon-greedy policy\n",
    "        if np.random.rand() <= epsilon:\n",
    "            actions = np.random.randint(2, size=(N_agents,))  # Random actions\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                actions_list = []\n",
    "                for i_agent in range(N_agents):\n",
    "                    model_name = 'agent'+str(i_agent)+'_model'\n",
    "                    q_values = agent_model[model_name](torch.tensor(state, dtype=torch.float32))\n",
    "                    actions_list.append(torch.argmax(q_values).item())\n",
    "                actions = np.array(actions_list)\n",
    "                \n",
    "        if all(action == 0 for action in actions):\n",
    "            non_zero_action = np.random.randint(N_agents)\n",
    "            actions[non_zero_action] = 1\n",
    "        \n",
    "        # Take actions and observe next states, rewards, done\n",
    "        next_state, done = env.step(actions)\n",
    "        rewards = env.get_rewards(predictor_model,X_stand_df,Y_stand_df)\n",
    "\n",
    "        # Update Q-values for each agent\n",
    "        for i, (model, optimizer, reward) in enumerate(zip(agent_model.values(),\n",
    "                                                           agent_optimizer.values(),\n",
    "                                                           rewards)):\n",
    "            q_values_next = model(torch.tensor(next_state, dtype=torch.float32))\n",
    "            target = reward + gamma * torch.max(q_values_next)\n",
    "\n",
    "            q_values = model(torch.tensor(state, dtype=torch.float32))\n",
    "            loss = nn.functional.mse_loss(q_values[actions[i]], target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_rewards[i] += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > 0.01:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Print episode results\n",
    "    if (episode + 1) % 1 == 0:\n",
    "        print(f\"Episode: {episode + 1}, state: {state}, Total Rewards: {total_rewards[0]}, Epsilon: {epsilon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad0ea0-4d70-41a8-9bb6-40dc60b8b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save QNetwork Models\n",
    "for i_agent in range(N_agents):\n",
    "    model_name = 'agent'+str(i_agent)+'_model'\n",
    "    saveModel_filename = output_dir+model_name+'.pt'\n",
    "    torch.save(agent_model[model_name].state_dict(), saveModel_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a2339-18fd-49f3-b6a6-e2489f959719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26530d9-7a41-4ac5-805b-92abbcd373d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the trained agents\n",
    "state = env.reset()\n",
    "total_rewards = np.zeros(N_agents)\n",
    "\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        actions_list = []\n",
    "        for i_agent in range(N_agents):\n",
    "            model_name = 'agent'+str(i_agent)+'_model'\n",
    "            q_values = agent_model[model_name](torch.tensor(state, dtype=torch.float32))\n",
    "            actions_list.append(torch.argmax(q_values).item())        \n",
    "        actions = np.array(actions_list)\n",
    "\n",
    "    next_state, done = env.step(actions)\n",
    "    rewards, feature_importance_dict_rl, mse_rl = env.get_rewards_test(predictor_model,X_stand_df,Y_stand_df)    \n",
    "    total_rewards += rewards  \n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "importance_df_rl = pd.DataFrame.from_dict(data=feature_importance_dict_rl, orient='index')\n",
    "importance_df_rl.to_csv(output_dir+'rl.csv')\n",
    "mse_df_rl = pd.DataFrame({'MSE_RL':[mse_rl]})   \n",
    "mse_df_rl.to_csv(output_dir+'rl_mse.csv')\n",
    "print(f\"Test Total Rewards: {total_rewards}, state: {state}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a409e-b8b5-4077-9398-8d0a766c5205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the trained agents\n",
    "state = env.reset()\n",
    "total_rewards = np.zeros(N_agents)\n",
    "\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        actions_list = []\n",
    "        for i_agent in range(N_agents):\n",
    "            model_name = 'agent'+str(i_agent)+'_model'\n",
    "            q_values = agent_model[model_name](torch.tensor(state, dtype=torch.float32))\n",
    "            actions_list.append(torch.argmax(q_values).item())        \n",
    "        actions = np.array(actions_list)\n",
    "\n",
    "    next_state, done = env.step(actions)\n",
    "    rewards, feature_importance_dict_rl, mse_rl = env.get_rewards_test(predictor_model,X_test_df,Y_test_df)    \n",
    "    total_rewards += rewards  \n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "importance_df_rl = pd.DataFrame.from_dict(data=feature_importance_dict_rl, orient='index')\n",
    "importance_df_rl.to_csv(output_dir+'rl_test.csv')\n",
    "mse_df_rl = pd.DataFrame({'MSE_RL':[mse_rl]})   \n",
    "mse_df_rl.to_csv(output_dir+'rl_mse_test.csv')\n",
    "print(f\"Test Total Rewards: {total_rewards}, state: {state}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c443e58-7e6d-4971-ac2c-9282d849fd63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
