{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34042145-e9e9-410a-a810-61d3b1e0d107",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maitreyeesharma/opt/anaconda3/envs/torch/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/maitreyeesharma/opt/anaconda3/envs/torch/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: /Users/maitreyeesharma/opt/anaconda3/envs/torch/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in: /Users/maitreyeesharma/opt/anaconda3/envs/torch/lib/python3.11/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# This file costructs surrogate models for the input datasets\n",
    "import numpy as np     \n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Sklearn modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Torch specific module imports\n",
    "import torch\n",
    "import gpytorch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import functional as F\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)    \n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# User defined files and classes\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.ReLMM.read_data import Inputs\n",
    "from expert_trainers import expert\n",
    "import utils_dataset as utilsd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77b0114c-fa21-4533-b342-a5ea2a3fa330",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainNeuralNetwork(nn.Module):\n",
    "    def __init__(self,in_features,out_variables,num_nodes):\n",
    "        super(MainNeuralNetwork,self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        # self.layer1 = nn.Linear(in_features, num_nodes,bias=True)\n",
    "        # self.layer2 = nn.Linear(num_nodes,out_variables,bias=True)\n",
    "        self.layer1 = nn.Linear(in_features, num_nodes,bias=True)\n",
    "        self.layer2 = nn.Linear(num_nodes, num_nodes,bias=True)\n",
    "        self.layer3 = nn.Linear(num_nodes,out_variables,bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        layer1_out = F.relu(self.layer1(x))\n",
    "        layer2_out = F.relu(self.layer2(layer1_out))\n",
    "        output = self.layer3(layer2_out)\n",
    "        \n",
    "        # layer1_out = F.relu(self.layer1(x)) #F.linear(self.layer1(x))\n",
    "        # output = self.layer2(layer1_out)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class TargetNeuralNetwork(nn.Module):\n",
    "    def __init__(self,in_features,out_variables,num_nodes):\n",
    "        super(TargetNeuralNetwork,self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        # self.layer1 = nn.Linear(in_features, num_nodes,bias=True)\n",
    "        # self.layer2 = nn.Linear(num_nodes,out_variables,bias=True)\n",
    "        self.layer1 = nn.Linear(in_features, num_nodes,bias=True)\n",
    "        self.layer2 = nn.Linear(num_nodes, num_nodes,bias=True)\n",
    "        self.layer3 = nn.Linear(num_nodes,out_variables,bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        layer1_out = F.relu(self.layer1(x))\n",
    "        layer2_out = F.relu(self.layer2(layer1_out))\n",
    "        output = self.layer3(layer2_out)\n",
    "        \n",
    "        # layer1_out = F.relu(self.layer1(x)) #F.linear(self.layer1(x))\n",
    "        # output = self.layer2(layer1_out)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class Train_NN():\n",
    "    \n",
    "    def __init__(self):\n",
    "        print('Starting training')\n",
    "        \n",
    "    def train_loop(self, dataloader, model, loss_fn, optimizer,lambda1,lambda2):\n",
    "        \n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        train_loss = 0.0\n",
    "        l1_regularization, l2_regularization = 0.0, 0.0\n",
    "        \n",
    "        for batch, sample_batched in enumerate(dataloader):\n",
    "            # Compute prediction and loss\n",
    "            X = sample_batched['in_features']\n",
    "            y = sample_batched['labels']\n",
    "            var = sample_batched['variance']\n",
    "            pred = model(X)\n",
    "            train_loss += loss_fn(pred, y).item()\n",
    "            pred_loss = loss_fn(pred, y)\n",
    "            \n",
    "            all_linear1_params = torch.cat([x.view(-1) for x in model.layer1.parameters()])\n",
    "            all_linear2_params = torch.cat([x.view(-1) for x in model.layer2.parameters()])\n",
    "            all_linear3_params = torch.cat([x.view(-1) for x in model.layer3.parameters()])\n",
    "            l1_regularization = lambda1 * (torch.norm(all_linear1_params, 1)+torch.norm(all_linear2_params, 1)+torch.norm(all_linear3_params, 1))\n",
    "            l2_regularization = lambda2 * (torch.norm(all_linear1_params, 2)+torch.norm(all_linear2_params, 2)+torch.norm(all_linear3_params, 2))\n",
    "\n",
    "            # l1_regularization = lambda1 * (torch.norm(all_linear1_params, 1)+torch.norm(all_linear2_params, 1))\n",
    "            # l2_regularization = lambda2 * (torch.norm(all_linear1_params, 2)+torch.norm(all_linear2_params, 2))\n",
    "\n",
    "            loss = pred_loss + l1_regularization + l2_regularization \n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /=num_batches\n",
    "        return train_loss\n",
    "\n",
    "\n",
    "    def test_loop(self, dataloader, model, loss_fn):\n",
    "        size = len(dataloader.dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        test_loss, correct = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sample_batched in dataloader:\n",
    "                X = sample_batched['in_features']\n",
    "                y = sample_batched['labels']  \n",
    "                var = sample_batched['variance']\n",
    "                pred = model(X)\n",
    "                test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        correct /= size\n",
    "        # print(f\"Test Error Avg loss: {test_loss:>8f} \\n\")\n",
    "        return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c34eaa6-e5a9-44b0-ba19-dd37fa3f28b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_agents(num_features,num_nodes,in_features,out_variables,learning_rate = 1e-3):\n",
    "    \n",
    "    model_main = {}\n",
    "    model_target = {}\n",
    "    optimizer_dict = {}\n",
    "    \n",
    "    # NN Policies for Q-Learning\n",
    "    for iFeature in range(0,num_features):\n",
    "        agent_name = 'agent_'+str(iFeature)\n",
    "        model_main[agent_name] = MainNeuralNetwork(in_features,out_variables,num_nodes)\n",
    "        model_target[agent_name] = TargetNeuralNetwork(in_features,out_variables,num_nodes)\n",
    "        optimizer_dict[agent_name] = torch.optim.Adam(model_main[agent_name].parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initializing loss and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    return loss_fn, optimizer_dict, model_main, model_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a2f4d64-b6c7-44ef-89f7-dc54e7691dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agents(replay_memory, agent_main, agent_target, loss_fn, optimizer, train_NN, predict_NN, saveModel_NN, saveModel_filename, test_size, epochs = 1000):\n",
    "\n",
    "    min_replay_size = 5\n",
    "    if len(replay_memory) < MIN_REPLAY_SIZE:\n",
    "        return\n",
    "    \n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_stand_training, Y_stand_training, test_size=test_size,random_state=40)\n",
    "    Var_train = torch.ones(len(Y_train)) \n",
    "    Var_test = torch.ones(len(Y_test)) \n",
    "        \n",
    "    # NN parameters\n",
    "    batch_size = 1\n",
    "    l1 = 1e-5\n",
    "    l2 = 1e-5\n",
    " \n",
    "    if (train_NN):\n",
    "        # Dataloader for pytorch\n",
    "        train_data = utilsd.InputDataset(X_train,Y_train,Var_train,descriptors)\n",
    "        test_data = utilsd.InputDataset(X_test,Y_test,Var_test,descriptors)\n",
    "\n",
    "        train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        user_training = Train_NN()\n",
    "\n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "        for t in range(epochs):\n",
    "            train_loss_epoch = user_training.train_loop(train_dataloader, agent_main, loss_fn, optimizer, l1, l2)\n",
    "            test_loss_epoch = user_training.test_loop(test_dataloader, agent_main, loss_fn)\n",
    "            train_loss.append(train_loss_epoch)\n",
    "            test_loss.append(test_loss_epoch)\n",
    "            if ((t+1)%100 == 0):\n",
    "                print(f\"Epoch {t+1}---> training error: {train_loss_epoch:>7f}, val error: {test_loss_epoch:>7f}\")\n",
    "            if((t+1)%10 == 0):\n",
    "                agent_target.load_state_dict(agent_main.state_dict())\n",
    "                # model_target.eval()\n",
    "                # output_target = model_target(X_train)\n",
    "                \n",
    "        fig, ax = plt.subplots(figsize=(6,4))\n",
    "        ax.plot(range(epochs),train_loss, label=f'Training Error,{train_loss_epoch:>7f}')\n",
    "        ax.plot(range(epochs),test_loss, label=f'Validation Error,{test_loss_epoch:>7f}')\n",
    "        ax.set_xlabel('Num. of epochs')\n",
    "        ax.set_ylabel('MSE Loss')\n",
    "        plt.legend()\n",
    "        print(\"NN training Done!\")\n",
    "        \n",
    "        if saveModel_NN:\n",
    "            torch.save(model.state_dict(), saveModel_filename)\n",
    "        \n",
    "    return\n",
    "\n",
    "def predict_agents(X_predict,agent_main):\n",
    "    X_predict = torch.tensor(X_predict).to(torch.float32)\n",
    "    agent_main.eval()\n",
    "\n",
    "    target_output = agent_main(X_predict)\n",
    "\n",
    "    return target_output\n",
    "\n",
    "class environment():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_space = [0, 1]\n",
    "        \n",
    "    def step(self,action):\n",
    "        state = torch.tensor(action).to(torch.int32)\n",
    "        return state\n",
    "    \n",
    "    def compute_accuracy_reward_DT(self,state,X,Y):\n",
    "    \n",
    "        predAccuracy_reward = random.sample([1,2,3,4,5], 1)\n",
    "        print(predAccuracy_reward)\n",
    "\n",
    "        return predAccuracy_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1cdf6941-3403-4eb9-8680-20d50872ec05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data for the input dataset type:  PerovAlloys\n",
      "0\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "[2]\n",
      "0\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "[1]\n",
      "tensor([ 0.0446, -0.1929], grad_fn=<AddBackward0>)\n",
      "0\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "[4]\n",
      "0\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "[1]\n",
      "0\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "[2]\n",
      "1\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "[5]\n",
      "1\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "[5]\n",
      "tensor([ 0.0525, -0.1658], grad_fn=<AddBackward0>)\n",
      "0\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "[3]\n",
      "1\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "[5]\n",
      "1\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "[4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nt/mld7___92p5cvk83b67390940000gn/T/ipykernel_4400/980236341.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_predict = torch.tensor(X_predict).to(torch.float32)\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    \n",
    "    run_folder = '/Users/maitreyeesharma/WORKSPACE/PostDoc/Chemistry/SPIRAL/codes/RL/RL_FS/'\n",
    "    \n",
    "    # Read and preprocess input data\n",
    "    input_data=inputs()\n",
    "    X_stand_DT, Y_stand_DT, descriptors = input_data.read_inputs()\n",
    "    \n",
    "    env = environment()\n",
    "    \n",
    "    # RL training inputs\n",
    "    train_episodes = 10\n",
    "    min_replay_size = 5\n",
    "    num_batch = 10\n",
    "    epsilon = 1 # Epsilon-greedy algorithm in initialized at 1 meaning every step is random at the start\n",
    "    max_epsilon = 1 # You can't explore more than 100% of the time\n",
    "    min_epsilon = 0.01 # At a minimum, we'll always explore 1% of the time\n",
    "    decay = 0.01\n",
    "    \n",
    "    # Initialize and instantiate feature agents, each agent have 8 neurons in the hidden layer\n",
    "    num_nodes = 8\n",
    "    num_features = np.size(descriptors)\n",
    "    in_features = np.size(descriptors) ## State vector size\n",
    "    out_variables = 2\n",
    "    loss_fn, optimizer_dict, model_main, model_target = instantiate_agents(num_features,num_nodes,in_features,out_variables,learning_rate = 1e-3)\n",
    "    \n",
    "    steps_to_update_target_model = 0\n",
    "    observation = torch.ones(in_features,dtype=torch.int32)\n",
    "    replay_memory = []\n",
    "    \n",
    "    for episode in range(train_episodes):\n",
    "        total_training_rewards = 0\n",
    "        # done = False\n",
    "        # while not done:\n",
    "        steps_to_update_target_model += 1\n",
    "        random_number = np.random.rand()\n",
    "        # 2. Explore using the Epsilon Greedy Exploration Strategy\n",
    "        if random_number <= epsilon:\n",
    "            # Explore\n",
    "            action = random.sample(env.action_space, 1)[0]\n",
    "        else:\n",
    "            # Exploit best known action\n",
    "            predicted_Qvalues = predict_agents(current_state,model_main['agent_0'])\n",
    "            print(predicted_Qvalues)\n",
    "            action = torch.argmax(predicted_Qvalues).item()\n",
    "        \n",
    "        new_observation_for_iFeature = env.step(action)\n",
    "        # Collect all new observations to create new state\n",
    "        new_state = torch.cat((new_observation_for_iFeature.reshape(1),current_state[1:]), dim=0)\n",
    "        reward = env.compute_accuracy_reward_DT(new_state,X_stand_DT, Y_stand_DT)\n",
    "        replay_memory.append([current_state, action, reward, new_state])\n",
    "\n",
    "#             # 3. Update the Main Network using the Bellman Equation\n",
    "#             if steps_to_update_target_model % 4 == 0 or done:\n",
    "#                 train(env, replay_memory, model, target_model, done)\n",
    "                # train_agents(state_input_training, qvalue_training, model_main[iAgent], model_target[iAgent], loss_fn, optimizer_dict[iAgent], train_NN, predict_NN, saveModel_NN, saveModel_filename, test_size)\n",
    "\n",
    "        current_state = new_state\n",
    "\n",
    "#                 if steps_to_update_target_model >= 100:\n",
    "#                     print('Copying main network weights to the target network weights')\n",
    "#                     target_model.set_weights(model.get_weights())\n",
    "#                     steps_to_update_target_model = 0\n",
    "#                 break\n",
    "\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40abb5f-a02a-422e-8805-560024893483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e05620-dda5-4c21-a250-e463b6a61d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e72ec-58a5-49e4-8f58-1ed51edf5f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76fa461-38a4-4fe3-a403-a99aa884d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    \n",
    "    run_folder = '/Users/maitreyeesharma/WORKSPACE/PostDoc/Chemistry/SPIRAL/codes/RL/RL_FS/'\n",
    "    \n",
    "    # Read and preprocess input data\n",
    "    input_data=inputs()\n",
    "    X_stand, Y_stand, descriptors = input_data.read_inputs()\n",
    "\n",
    "    # Initialize feature agents\n",
    "    num_nodes = 8\n",
    "    num_features = np.size(descriptors)\n",
    "    in_features = np.size(descriptors)\n",
    "    out_variables = 2\n",
    "    loss_fn, optimizer_dict, model_main, model_target = instantiate_agents(num_features,num_nodes,in_features,out_variables,learning_rate = 1e-3)\n",
    "    \n",
    "    # Initialize expert trainer\n",
    "    expert_trainer = expert()\n",
    "    \n",
    "    # Game data to train policy networks\n",
    "    num_game_instances = 50\n",
    "    state = []\n",
    "    reward = []\n",
    "    for iGame in range(0,num_game_instances):\n",
    "        sampled_descriptors = random.sample(list(descriptors), 5)\n",
    "        X_stand_training = pd.DataFrame(X_stand, columns=sampled_descriptors)\n",
    "        Y_stand_training = Y_stand\n",
    "\n",
    "        # XGboost data for training policy\n",
    "        feature_importance_dict, reward_game = expert_trainer.expert_xgboost(X_stand_training,Y_stand_training,sampled_descriptors,onlyTopChoices=False)\n",
    "\n",
    "        state_game = []\n",
    "        for idescriptor in range(0,len(list(descriptors))):\n",
    "            if descriptors[idescriptor] in feature_importance_dict.keys():\n",
    "                state_game.append(int(1))\n",
    "            else:\n",
    "                state_game.append(int(0))\n",
    "        \n",
    "        state.append(state_game)\n",
    "        reward.append(reward_game)\n",
    "    \n",
    "    # Training agents using 50 feature subsets\n",
    "    for iAgent in model_main.keys():\n",
    "        absent_index = 0\n",
    "        absent_reward = 0.0\n",
    "        present_index = 0 \n",
    "        present_reward = 0.0\n",
    "        qvalue_training = np.zeros((len(reward),2))\n",
    "            \n",
    "        feature_index = int(iAgent.split('agent_')[1])\n",
    "        train_NN = True\n",
    "        saveModel_NN = False \n",
    "        predict_NN = False\n",
    "        test_size = 0.1\n",
    "        saveModel_filename = '../RL_FS_output/alloys_' + str(int(test_size*100)) + 'test_'+str(num_nodes)+'nodes_l1_1em3.pt'\n",
    "        state_input_training = state\n",
    "        \n",
    "        # First game:\n",
    "        if state[0][feature_index] == 0:\n",
    "            absent_index += 1\n",
    "            absent_reward = reward[0]\n",
    "            qvalue_training[0][0] = reward[0]\n",
    "        elif state[0][feature_index] == 1:\n",
    "            present_index += 1\n",
    "            present_reward = reward[0]\n",
    "            qvalue_training[0][1] = reward[0]\n",
    "        \n",
    "        # Remaining games:\n",
    "        for iGame in range(1,num_game_instances):\n",
    "            if state[iGame][feature_index] == 0:\n",
    "                absent_index += 1\n",
    "                absent_reward = reward[iGame]\n",
    "                qvalue_training[iGame][0] = absent_reward\n",
    "            elif state[iGame][feature_index] == 1:\n",
    "                present_index += 1\n",
    "                present_reward = reward[iGame]\n",
    "                qvalue_training[iGame][1] = present_reward\n",
    "   \n",
    "        \n",
    "        train_agents(state_input_training, qvalue_training, model_main[iAgent], model_target[iAgent], loss_fn, optimizer_dict[iAgent], train_NN, predict_NN, saveModel_NN, saveModel_filename, test_size)\n",
    " \n",
    "    # Predicting using agents   \n",
    "    num_game_instances = 10\n",
    "    state = []\n",
    "    reward = []\n",
    "    for iGame in range(0,num_game_instances):\n",
    "        sampled_descriptors = random.sample(list(descriptors), 5)\n",
    "        X_stand_training = pd.DataFrame(X_stand, columns=sampled_descriptors)\n",
    "        Y_stand_training = Y_stand\n",
    "\n",
    "        # XGboost data for training policy\n",
    "        feature_importance_dict, reward_game = expert_trainer.expert_xgboost(X_stand_training,Y_stand_training,sampled_descriptors,onlyTopChoices=False)\n",
    "\n",
    "        state_game = []\n",
    "        for idescriptor in range(0,len(list(descriptors))):\n",
    "            if descriptors[idescriptor] in feature_importance_dict.keys():\n",
    "                state_game.append(int(1))\n",
    "            else:\n",
    "                state_game.append(int(0))\n",
    "        \n",
    "        state.append(state_game)\n",
    "        reward.append(reward_game)\n",
    "        print(state_game, reward_game)\n",
    "        \n",
    "    for iAgent in model_main.keys():\n",
    "        feature_index = int(iAgent.split('agent_')[1])\n",
    "        train_NN = True\n",
    "        saveModel_NN = False \n",
    "        predict_NN = False\n",
    "        test_size = 0.1\n",
    "        saveModel_filename = '../RL_FS_output/alloys_' + str(int(test_size*100)) + 'test_'+str(num_nodes)+'nodes_l1_1em3.pt'\n",
    "        state_input_training = state\n",
    "\n",
    "        target_output = predict_agents(state,model_main[iAgent])\n",
    "        print(target_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
