{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3933a-cd58-4074-9fa9-f9a9ec18b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# User defined files and classes\n",
    "from ReLMM.read_data import Inputs\n",
    "from ReLMM.utils_dataset import standardize_data\n",
    "from ReLMM.environment import Environment\n",
    "from ReLMM.qlearning import QNetwork\n",
    "from ReLMM.predictors import Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3771a5d8-ccfd-41cd-9983-ad90a1e0aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the input json file with dataset filename and path information\n",
    "with open('inputs.json', \"r\") as f:\n",
    "    input_dict = json.load(f)\n",
    "\n",
    "input_type = input_dict['InputType']\n",
    "input_path = input_dict['InputPath']\n",
    "input_file = input_dict['InputFile']\n",
    "output_dir = input_dict['OutputDirectory']\n",
    "\n",
    "# Create a new output directory if it does not exist\n",
    "isExist = os.path.exists(output_dir)\n",
    "if not isExist:\n",
    "    os.makedirs(output_dir)\n",
    "    print(\"The new directory is created!\", output_dir)\n",
    "\n",
    "input_data = Inputs(input_type=input_type,\n",
    "                    input_path=input_path,\n",
    "                    input_file=input_file)\n",
    "\n",
    "input_data = Inputs(input_type=\"SynthData\",\n",
    "                    input_path=\"../synthetic_dataset\",\n",
    "                    input_file=\"synthetic_data_randomSamples_200_nonlinearf5.csv\")\n",
    "\n",
    "X_data, Y_data, descriptors = input_data.read_inputs()\n",
    "X_stand_all, X_stand_df_all, scalerX = standardize_data(X_data)\n",
    "Y_stand_all, Y_stand_df_all, scalerY = standardize_data(pd.DataFrame({'target':Y_data[:,0]}))\n",
    "X_stand_df, X_test_df, Y_stand_df, Y_test_df = train_test_split(X_stand_df_all, Y_stand_df_all, test_size=0.1, random_state=0)\n",
    "X_stand, X_test, Y_stand, Y_test = train_test_split(X_stand_all, Y_stand_all, test_size=0.1, random_state=0)\n",
    "\n",
    "# Dataset parameters\n",
    "total_num_features = len(descriptors)\n",
    "\n",
    "# Environment parameters\n",
    "state_size = total_num_features  # Size of the state space\n",
    "N_agents = total_num_features # Number of agents\n",
    "action_size = 2  # Number of possible actions\n",
    "N_steps = 100 # Number of steps to take per episode\n",
    "predictor_model = Predictors()\n",
    "\n",
    "# Hyperparameters\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_decay = 0.995  # Decay rate of exploration\n",
    "gamma = 0.95  # Discount factor\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize environment and Q-networks for each agent\n",
    "env = Environment(state_size,action_size,N_agents,N_steps)\n",
    "agent_model = {}\n",
    "agent_optimizer = {}\n",
    "agent_qvalue = {}\n",
    "\n",
    "for i_agent in range(N_agents):\n",
    "    model_name = 'agent'+str(i_agent)+'_model'\n",
    "    optimizer_name = 'agent'+str(i_agent)+'_optimizer'\n",
    "    agent_model[model_name] = QNetwork(env.state_size, env.action_size)\n",
    "    agent_optimizer[optimizer_name] = optim.Adam(agent_model[model_name].parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_episodes = 1000\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    total_rewards = np.zeros(N_agents) # Total rewards for agents\n",
    "    \n",
    "    while True:\n",
    "        # Agents choose actions using epsilon-greedy policy\n",
    "        if np.random.rand() <= epsilon:\n",
    "            actions = np.random.randint(2, size=(N_agents,))  # Random actions\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                actions_list = []\n",
    "                for i_agent in range(N_agents):\n",
    "                    model_name = 'agent'+str(i_agent)+'_model'\n",
    "                    q_values = agent_model[model_name](torch.tensor(state, dtype=torch.float32))\n",
    "                    actions_list.append(torch.argmax(q_values).item())\n",
    "                actions = np.array(actions_list)\n",
    "                \n",
    "        if all(action == 0 for action in actions):\n",
    "            non_zero_action = np.random.randint(N_agents)\n",
    "            actions[non_zero_action] = 1\n",
    "        \n",
    "        # Take actions and observe next states, rewards, done\n",
    "        next_state, done = env.step(actions)\n",
    "        rewards = env.get_rewards(predictor_model,X_stand_df,Y_stand_df)\n",
    "\n",
    "        # Update Q-values for each agent\n",
    "        for i, (model, optimizer, reward) in enumerate(zip(agent_model.values(),\n",
    "                                                           agent_optimizer.values(),\n",
    "                                                           rewards)):\n",
    "            q_values_next = model(torch.tensor(next_state, dtype=torch.float32))\n",
    "            target = reward + gamma * torch.max(q_values_next)\n",
    "\n",
    "            q_values = model(torch.tensor(state, dtype=torch.float32))\n",
    "            loss = nn.functional.mse_loss(q_values[actions[i]], target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_rewards[i] += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > 0.01:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Print episode results\n",
    "    if (episode + 1) % 1 == 0:\n",
    "        print(f\"Episode: {episode + 1}, state: {state}, Total Rewards: {total_rewards[0]}, Epsilon: {epsilon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad0ea0-4d70-41a8-9bb6-40dc60b8b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save QNetwork Models\n",
    "for i_agent in range(N_agents):\n",
    "    model_name = 'agent'+str(i_agent)+'_model'\n",
    "    saveModel_filename = f'{output_dir}/{model_name}.pt'\n",
    "    torch.save(agent_model[model_name].state_dict(), saveModel_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26530d9-7a41-4ac5-805b-92abbcd373d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the trained agents\n",
    "state = env.reset()\n",
    "total_rewards = np.zeros(N_agents)\n",
    "\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        actions_list = []\n",
    "        for i_agent in range(N_agents):\n",
    "            model_name = 'agent'+str(i_agent)+'_model'\n",
    "            q_values = agent_model[model_name](torch.tensor(state, dtype=torch.float32))\n",
    "            actions_list.append(torch.argmax(q_values).item())        \n",
    "        actions = np.array(actions_list)\n",
    "\n",
    "    next_state, done = env.step(actions)\n",
    "    rewards, feature_importance_dict_rl, mse_rl = env.get_rewards_test(predictor_model,X_stand_df,Y_stand_df)    \n",
    "    total_rewards += rewards  \n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "importance_df_rl = pd.DataFrame.from_dict(data=feature_importance_dict_rl, orient='index')\n",
    "importance_df_rl.to_csv(output_dir+'rl.csv')\n",
    "print(f\"Test Total Rewards: {total_rewards}, state: {state}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReLMM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
