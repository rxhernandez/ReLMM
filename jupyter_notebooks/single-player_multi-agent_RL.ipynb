{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dd3933a-cd58-4074-9fa9-f9a9ec18b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# User defined files and classes\n",
    "import sys\n",
    "from read_data import inputs\n",
    "import utils_dataset as utilsd\n",
    "from environment import Environment\n",
    "from qlearning import QNetwork\n",
    "from predictor_models import predictor_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed81dc-10bc-475f-96a5-a8de8435eaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3771a5d8-ccfd-41cd-9983-ad90a1e0aaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data for the input dataset type:  Gryffin\n",
      "This class contains the predictor models to generate rewards for agents\n",
      "Episode: 1, state: [1 1 1 0 0 1 1 1 0 1 1 1 0 0], Total Rewards: 1042.390102724037, Epsilon: 0.995\n",
      "Episode: 2, state: [0 0 1 1 0 1 1 1 1 1 1 0 1 0], Total Rewards: 988.8771750716556, Epsilon: 0.990025\n",
      "Episode: 3, state: [0 0 1 0 0 1 0 1 0 1 0 0 1 0], Total Rewards: 979.5143832397449, Epsilon: 0.985074875\n",
      "Episode: 4, state: [1 0 0 0 1 0 0 1 0 1 0 1 1 0], Total Rewards: 1001.584056709807, Epsilon: 0.9801495006250001\n",
      "Episode: 5, state: [0 1 0 1 0 1 1 0 0 1 1 1 1 1], Total Rewards: 1045.6921844563403, Epsilon: 0.9752487531218751\n",
      "Episode: 6, state: [1 0 1 0 1 1 1 0 1 0 1 0 0 0], Total Rewards: 1077.9913839361263, Epsilon: 0.9703725093562657\n",
      "Episode: 7, state: [0 0 0 0 0 1 0 0 1 1 1 0 0 1], Total Rewards: 1001.7043400345137, Epsilon: 0.9655206468094844\n",
      "Episode: 8, state: [0 0 0 0 1 0 1 1 1 0 1 1 1 0], Total Rewards: 1053.3186913444574, Epsilon: 0.960693043575437\n",
      "Episode: 9, state: [0 0 0 1 0 1 0 0 1 1 0 0 1 0], Total Rewards: 1081.6964085201726, Epsilon: 0.9558895783575597\n",
      "Episode: 10, state: [0 0 1 1 1 1 0 0 0 0 1 0 0 1], Total Rewards: 1015.7971873942921, Epsilon: 0.9511101304657719\n",
      "Episode: 11, state: [1 0 1 0 0 0 1 0 0 0 1 1 1 0], Total Rewards: 1033.553117554931, Epsilon: 0.946354579813443\n",
      "Episode: 12, state: [0 1 1 1 0 1 1 0 0 1 1 0 0 0], Total Rewards: 1026.106709698085, Epsilon: 0.9416228069143757\n",
      "Episode: 13, state: [1 1 0 0 1 0 0 1 0 0 0 1 1 0], Total Rewards: 951.7201775303411, Epsilon: 0.9369146928798039\n",
      "Episode: 14, state: [0 0 0 1 0 0 0 1 1 1 1 0 0 1], Total Rewards: 1062.088622158888, Epsilon: 0.9322301194154049\n",
      "Episode: 15, state: [1 1 1 0 1 1 1 0 0 0 0 0 0 1], Total Rewards: 987.2378925003193, Epsilon: 0.9275689688183278\n",
      "Episode: 16, state: [0 1 0 0 0 0 0 1 1 1 0 1 0 0], Total Rewards: 1053.8565985043444, Epsilon: 0.9229311239742362\n",
      "Episode: 17, state: [1 0 0 0 1 1 1 1 0 0 1 0 1 1], Total Rewards: 1044.5536385618886, Epsilon: 0.918316468354365\n",
      "Episode: 18, state: [1 0 1 1 0 1 0 0 1 0 1 0 1 1], Total Rewards: 1022.3874234105295, Epsilon: 0.9137248860125932\n",
      "Episode: 19, state: [1 0 0 0 0 0 1 1 0 1 0 0 0 0], Total Rewards: 959.3125998095636, Epsilon: 0.9091562615825302\n",
      "Episode: 20, state: [1 1 1 1 1 0 0 1 1 0 0 0 0 1], Total Rewards: 1012.8164612074553, Epsilon: 0.9046104802746175\n",
      "Episode: 21, state: [0 1 1 1 0 1 1 1 0 1 1 1 0 1], Total Rewards: 1008.9036682187517, Epsilon: 0.9000874278732445\n",
      "Episode: 22, state: [1 1 0 0 1 1 0 0 0 0 0 1 0 1], Total Rewards: 955.8858533036489, Epsilon: 0.8955869907338783\n",
      "Episode: 23, state: [1 1 0 0 0 1 1 0 1 1 0 1 0 1], Total Rewards: 965.2498654837557, Epsilon: 0.8911090557802088\n",
      "Episode: 24, state: [0 1 1 0 1 0 1 1 1 1 0 0 1 1], Total Rewards: 985.7695392078083, Epsilon: 0.8866535105013078\n",
      "Episode: 25, state: [0 0 1 0 0 0 1 0 0 1 1 0 0 1], Total Rewards: 1027.6949861667165, Epsilon: 0.8822202429488013\n",
      "Episode: 26, state: [1 0 1 1 0 0 0 1 1 0 0 1 0 1], Total Rewards: 1082.0008038930894, Epsilon: 0.8778091417340573\n",
      "Episode: 27, state: [0 1 0 1 1 1 0 1 0 0 1 0 0 0], Total Rewards: 1099.3574005267092, Epsilon: 0.8734200960253871\n",
      "Episode: 28, state: [0 1 0 1 0 0 1 1 1 0 1 0 1 1], Total Rewards: 1034.369140932779, Epsilon: 0.8690529955452602\n",
      "Episode: 29, state: [0 1 0 1 1 0 0 1 1 0 1 0 1 1], Total Rewards: 1025.2571195097469, Epsilon: 0.8647077305675338\n",
      "Episode: 30, state: [0 0 0 1 1 1 0 0 0 1 0 0 1 1], Total Rewards: 1030.7010239591334, Epsilon: 0.8603841919146962\n",
      "Episode: 31, state: [1 1 1 1 1 1 1 1 0 1 0 1 1 1], Total Rewards: 1026.5064744170822, Epsilon: 0.8560822709551227\n",
      "Episode: 32, state: [0 0 1 1 0 0 1 1 1 1 0 1 0 1], Total Rewards: 1061.5983848041449, Epsilon: 0.851801859600347\n",
      "Episode: 33, state: [0 1 0 1 0 0 0 1 0 0 1 1 1 0], Total Rewards: 1052.303351317515, Epsilon: 0.8475428503023453\n",
      "Episode: 34, state: [1 0 1 0 1 1 1 1 0 0 1 0 0 0], Total Rewards: 1004.5713615766144, Epsilon: 0.8433051360508336\n",
      "Episode: 35, state: [0 0 1 0 0 1 0 0 0 1 0 1 1 0], Total Rewards: 979.5253155928505, Epsilon: 0.8390886103705794\n",
      "Episode: 36, state: [0 1 1 0 0 0 1 1 1 0 0 0 1 1], Total Rewards: 1054.9421671953028, Epsilon: 0.8348931673187264\n",
      "Episode: 37, state: [1 0 0 0 0 0 0 0 1 0 1 1 1 1], Total Rewards: 987.1859878303244, Epsilon: 0.8307187014821328\n",
      "Episode: 38, state: [0 0 0 0 0 1 0 0 1 1 1 1 1 1], Total Rewards: 1035.034726367599, Epsilon: 0.8265651079747222\n",
      "Episode: 39, state: [0 0 0 0 0 0 0 0 1 0 1 1 0 0], Total Rewards: 990.3678845280763, Epsilon: 0.8224322824348486\n",
      "Episode: 40, state: [1 0 1 0 0 0 1 1 0 0 0 1 0 1], Total Rewards: 960.118874401957, Epsilon: 0.8183201210226743\n",
      "Episode: 41, state: [1 0 1 0 0 0 1 1 0 1 1 1 1 1], Total Rewards: 1010.1703009872472, Epsilon: 0.8142285204175609\n",
      "Episode: 42, state: [0 1 1 0 1 1 1 0 1 0 1 0 0 0], Total Rewards: 1015.1074320125923, Epsilon: 0.810157377815473\n",
      "Episode: 43, state: [0 1 0 0 0 1 0 1 0 0 1 0 0 1], Total Rewards: 984.6451952201231, Epsilon: 0.8061065909263957\n",
      "Episode: 44, state: [1 1 0 1 0 1 1 1 1 1 0 1 0 1], Total Rewards: 1075.9543970139948, Epsilon: 0.8020760579717637\n",
      "Episode: 45, state: [0 1 0 1 0 1 0 0 1 0 1 1 0 1], Total Rewards: 1063.8103523172852, Epsilon: 0.798065677681905\n",
      "Episode: 46, state: [1 1 1 1 1 1 1 1 1 1 1 0 1 1], Total Rewards: 1027.581457728572, Epsilon: 0.7940753492934954\n",
      "Episode: 47, state: [0 0 1 0 0 0 1 0 0 1 0 0 0 0], Total Rewards: 978.4942131189033, Epsilon: 0.7901049725470279\n",
      "Episode: 48, state: [0 0 1 0 0 1 1 1 0 1 0 0 0 0], Total Rewards: 1079.8248320403159, Epsilon: 0.7861544476842928\n",
      "Episode: 49, state: [1 1 0 0 0 1 1 1 1 1 1 0 1 1], Total Rewards: 1090.9835070219374, Epsilon: 0.7822236754458713\n",
      "Episode: 50, state: [0 1 0 0 1 1 0 1 0 1 0 0 0 0], Total Rewards: 1021.7632344559798, Epsilon: 0.778312557068642\n",
      "Episode: 51, state: [1 0 0 1 0 0 1 0 0 0 0 0 1 1], Total Rewards: 1028.9018638477348, Epsilon: 0.7744209942832988\n",
      "Episode: 52, state: [1 1 0 1 1 0 1 1 0 1 1 1 0 1], Total Rewards: 998.3595304482358, Epsilon: 0.7705488893118823\n",
      "Episode: 53, state: [1 1 0 1 1 1 1 1 0 1 1 1 0 0], Total Rewards: 1068.356847617905, Epsilon: 0.7666961448653229\n",
      "Episode: 54, state: [0 0 1 0 1 0 1 1 0 0 0 1 1 1], Total Rewards: 1028.564025227272, Epsilon: 0.7628626641409962\n",
      "Episode: 55, state: [0 0 0 1 0 0 0 1 1 1 0 1 0 1], Total Rewards: 981.6066293688104, Epsilon: 0.7590483508202912\n",
      "Episode: 56, state: [1 0 1 0 0 0 1 1 1 0 0 1 1 1], Total Rewards: 1076.906257010347, Epsilon: 0.7552531090661897\n",
      "Episode: 57, state: [1 0 0 1 0 1 1 0 1 1 1 0 0 1], Total Rewards: 1028.4707916437424, Epsilon: 0.7514768435208588\n",
      "Episode: 58, state: [1 1 0 1 1 1 1 1 1 1 0 1 1 0], Total Rewards: 1010.0518411304149, Epsilon: 0.7477194593032545\n",
      "Episode: 59, state: [0 0 0 1 1 1 0 0 1 1 1 0 1 0], Total Rewards: 1088.6494798017022, Epsilon: 0.7439808620067382\n",
      "Episode: 60, state: [1 1 1 1 1 1 1 0 1 1 1 0 1 0], Total Rewards: 1082.6066355160388, Epsilon: 0.7402609576967045\n",
      "Episode: 61, state: [1 1 0 0 1 0 0 1 0 0 0 0 0 1], Total Rewards: 998.2872355658714, Epsilon: 0.736559652908221\n",
      "Episode: 62, state: [0 1 1 0 0 0 0 0 1 0 0 1 0 1], Total Rewards: 991.7992763782361, Epsilon: 0.7328768546436799\n",
      "Episode: 63, state: [1 1 1 0 0 0 0 1 1 0 0 0 1 0], Total Rewards: 1012.3341292376659, Epsilon: 0.7292124703704616\n",
      "Episode: 64, state: [0 1 1 1 0 1 0 1 0 1 1 0 1 0], Total Rewards: 1045.362017513384, Epsilon: 0.7255664080186093\n",
      "Episode: 65, state: [0 0 0 1 1 0 1 1 0 1 0 1 0 0], Total Rewards: 1020.7999228892699, Epsilon: 0.7219385759785162\n",
      "Episode: 66, state: [0 0 1 1 1 0 0 0 1 0 1 1 1 1], Total Rewards: 1017.1102786575952, Epsilon: 0.7183288830986236\n",
      "Episode: 67, state: [0 1 1 1 1 1 1 1 1 0 0 1 0 1], Total Rewards: 1003.6214225554979, Epsilon: 0.7147372386831305\n",
      "Episode: 68, state: [1 0 1 1 0 1 1 1 0 0 0 0 1 1], Total Rewards: 1021.9146192962372, Epsilon: 0.7111635524897149\n",
      "Episode: 69, state: [0 0 0 1 1 1 0 0 0 1 0 0 1 1], Total Rewards: 1111.2111905388024, Epsilon: 0.7076077347272662\n",
      "Episode: 70, state: [0 0 0 1 1 0 0 1 0 1 0 1 0 1], Total Rewards: 1035.8491817510076, Epsilon: 0.7040696960536299\n",
      "Episode: 71, state: [1 0 0 0 0 0 1 1 1 0 1 0 1 0], Total Rewards: 965.6394499341982, Epsilon: 0.7005493475733617\n",
      "Episode: 72, state: [0 0 0 0 1 1 1 0 0 1 0 0 1 1], Total Rewards: 1085.5529211858407, Epsilon: 0.697046600835495\n",
      "Episode: 73, state: [1 1 1 0 0 0 1 1 0 1 0 1 0 1], Total Rewards: 1037.4579352160458, Epsilon: 0.6935613678313175\n",
      "Episode: 74, state: [1 0 1 0 0 0 0 0 1 1 1 0 0 0], Total Rewards: 1057.1537915935105, Epsilon: 0.6900935609921609\n",
      "Episode: 75, state: [1 0 0 0 0 1 0 0 0 1 1 0 1 0], Total Rewards: 1079.0213524310834, Epsilon: 0.6866430931872001\n",
      "Episode: 76, state: [1 0 0 0 0 0 0 1 0 1 1 0 1 0], Total Rewards: 1017.944487694407, Epsilon: 0.6832098777212641\n",
      "Episode: 77, state: [0 1 0 1 0 0 1 1 1 1 0 0 0 0], Total Rewards: 1032.4426976978327, Epsilon: 0.6797938283326578\n",
      "Episode: 78, state: [1 0 1 0 1 1 1 0 1 1 1 0 0 1], Total Rewards: 1038.5122707600424, Epsilon: 0.6763948591909945\n",
      "Episode: 79, state: [0 0 0 0 1 0 0 1 0 1 1 0 1 0], Total Rewards: 1040.56877343982, Epsilon: 0.6730128848950395\n",
      "Episode: 80, state: [1 0 0 0 0 1 1 1 1 0 1 1 0 0], Total Rewards: 990.9094880670191, Epsilon: 0.6696478204705644\n",
      "Episode: 81, state: [1 0 1 1 0 0 0 1 0 0 1 1 1 0], Total Rewards: 1065.60533185463, Epsilon: 0.6662995813682115\n",
      "Episode: 82, state: [0 0 0 0 1 0 1 1 1 0 1 1 1 0], Total Rewards: 1099.8181779458066, Epsilon: 0.6629680834613705\n",
      "Episode: 83, state: [1 1 0 1 0 0 0 0 1 0 0 1 1 0], Total Rewards: 1125.080826474595, Epsilon: 0.6596532430440636\n",
      "Episode: 84, state: [1 1 0 1 0 0 1 1 0 0 0 0 0 0], Total Rewards: 1051.3116505757555, Epsilon: 0.6563549768288433\n",
      "Episode: 85, state: [0 1 1 0 0 1 1 1 1 0 1 0 0 1], Total Rewards: 1029.485377993047, Epsilon: 0.653073201944699\n",
      "Episode: 86, state: [1 0 1 0 0 0 1 1 1 1 0 1 0 1], Total Rewards: 1087.3195080286316, Epsilon: 0.6498078359349755\n",
      "Episode: 87, state: [0 0 1 1 1 1 1 1 1 0 0 1 0 1], Total Rewards: 1037.719138102085, Epsilon: 0.6465587967553006\n",
      "Episode: 88, state: [1 1 0 1 1 0 0 1 1 0 0 0 1 1], Total Rewards: 1117.7978743221097, Epsilon: 0.6433260027715241\n",
      "Episode: 89, state: [1 0 0 0 0 0 1 1 1 1 1 1 1 0], Total Rewards: 1061.0279584330558, Epsilon: 0.6401093727576664\n",
      "Episode: 90, state: [0 0 0 0 1 0 1 0 0 0 1 1 0 1], Total Rewards: 1064.330620077473, Epsilon: 0.6369088258938781\n",
      "Episode: 91, state: [0 1 1 1 1 1 0 1 0 1 0 0 0 1], Total Rewards: 1034.8534160344248, Epsilon: 0.6337242817644086\n",
      "Episode: 92, state: [0 1 1 1 0 1 1 0 1 0 1 0 0 1], Total Rewards: 1078.686984579508, Epsilon: 0.6305556603555866\n",
      "Episode: 93, state: [0 0 0 0 0 1 0 1 1 0 0 1 0 0], Total Rewards: 1047.4317030238526, Epsilon: 0.6274028820538087\n",
      "Episode: 94, state: [0 1 0 0 1 1 1 0 1 1 0 0 1 1], Total Rewards: 1062.3364756142396, Epsilon: 0.6242658676435396\n",
      "Episode: 95, state: [0 1 1 1 0 1 1 1 1 0 1 1 1 0], Total Rewards: 1014.3124482015756, Epsilon: 0.6211445383053219\n",
      "Episode: 96, state: [1 0 0 0 0 0 1 1 0 1 0 0 0 0], Total Rewards: 1061.3692362483855, Epsilon: 0.6180388156137953\n",
      "Episode: 97, state: [0 1 0 0 1 0 0 0 0 1 1 0 0 1], Total Rewards: 1070.6253252276665, Epsilon: 0.6149486215357263\n",
      "Episode: 98, state: [0 0 0 1 1 1 1 0 1 1 0 0 1 1], Total Rewards: 1027.0703014805263, Epsilon: 0.6118738784280476\n",
      "Episode: 99, state: [0 1 1 1 0 1 0 1 0 1 1 1 0 1], Total Rewards: 1030.4971992491517, Epsilon: 0.6088145090359074\n",
      "Episode: 100, state: [0 1 0 0 1 1 1 0 1 0 1 1 1 1], Total Rewards: 1087.359901311658, Epsilon: 0.6057704364907278\n",
      "Episode: 101, state: [1 1 1 0 1 1 0 1 1 1 0 1 1 1], Total Rewards: 1061.3273916924538, Epsilon: 0.6027415843082742\n",
      "Episode: 102, state: [1 0 1 1 1 1 0 1 0 1 0 1 1 0], Total Rewards: 1105.4016265379325, Epsilon: 0.5997278763867329\n",
      "Episode: 103, state: [0 0 1 1 0 1 0 1 1 1 0 0 0 1], Total Rewards: 1108.5094889701152, Epsilon: 0.5967292370047992\n",
      "Episode: 104, state: [1 0 0 0 0 1 0 1 1 0 1 0 0 0], Total Rewards: 1030.6379916638052, Epsilon: 0.5937455908197752\n",
      "Episode: 105, state: [0 1 1 0 0 0 0 0 1 0 1 0 0 0], Total Rewards: 1056.499809798176, Epsilon: 0.5907768628656763\n",
      "Episode: 106, state: [1 1 1 1 0 1 0 1 0 0 1 0 1 0], Total Rewards: 1046.60759125423, Epsilon: 0.5878229785513479\n",
      "Episode: 107, state: [0 1 1 1 0 0 1 1 0 0 1 1 0 0], Total Rewards: 1093.2431695905973, Epsilon: 0.5848838636585911\n",
      "Episode: 108, state: [0 0 0 1 1 0 1 0 0 0 0 1 0 1], Total Rewards: 1071.6183250472734, Epsilon: 0.5819594443402982\n",
      "Episode: 109, state: [1 1 0 0 0 1 1 1 0 0 1 1 0 0], Total Rewards: 1084.9677833956955, Epsilon: 0.5790496471185967\n",
      "Episode: 110, state: [1 1 1 0 1 1 1 0 1 0 1 1 0 1], Total Rewards: 1067.4741315770725, Epsilon: 0.5761543988830038\n",
      "Episode: 111, state: [1 1 1 1 1 1 0 1 0 1 1 1 1 1], Total Rewards: 1065.2907763990502, Epsilon: 0.5732736268885887\n",
      "Episode: 112, state: [0 0 1 1 1 1 0 1 1 0 1 0 0 1], Total Rewards: 1044.8148581934431, Epsilon: 0.5704072587541458\n",
      "Episode: 113, state: [1 1 1 1 0 0 0 1 1 0 1 0 0 0], Total Rewards: 1065.5693942546925, Epsilon: 0.567555222460375\n",
      "Episode: 114, state: [0 0 0 1 1 0 1 0 0 1 1 1 1 0], Total Rewards: 1047.5501023398558, Epsilon: 0.5647174463480732\n",
      "Episode: 115, state: [1 1 0 0 1 1 1 0 0 0 1 0 1 0], Total Rewards: 1077.8960548850512, Epsilon: 0.5618938591163328\n",
      "Episode: 116, state: [1 0 1 1 0 1 0 1 0 0 1 0 0 0], Total Rewards: 1091.6447544680193, Epsilon: 0.5590843898207511\n",
      "Episode: 117, state: [1 0 1 1 1 0 1 1 1 0 0 0 1 0], Total Rewards: 1001.9382958249781, Epsilon: 0.5562889678716474\n",
      "Episode: 118, state: [0 1 0 1 1 1 0 0 0 0 0 1 1 1], Total Rewards: 1015.2600056880922, Epsilon: 0.5535075230322891\n",
      "Episode: 119, state: [1 0 0 1 0 1 0 1 0 1 0 0 1 1], Total Rewards: 1015.4412193919534, Epsilon: 0.5507399854171277\n",
      "Episode: 120, state: [0 1 1 0 0 0 1 1 0 0 1 1 1 0], Total Rewards: 1055.1832106373063, Epsilon: 0.547986285490042\n",
      "Episode: 121, state: [0 0 0 1 1 1 1 1 0 0 1 1 1 1], Total Rewards: 1054.8529768546114, Epsilon: 0.5452463540625918\n",
      "Episode: 122, state: [0 0 1 0 1 1 1 0 1 0 0 0 1 1], Total Rewards: 1090.8606721039423, Epsilon: 0.5425201222922789\n",
      "Episode: 123, state: [1 1 0 1 0 1 1 1 1 0 1 0 1 0], Total Rewards: 976.6878171804286, Epsilon: 0.5398075216808175\n",
      "Episode: 124, state: [0 0 1 0 1 0 0 0 1 0 1 1 0 1], Total Rewards: 1082.1426481958867, Epsilon: 0.5371084840724134\n",
      "Episode: 125, state: [0 0 1 0 0 0 0 1 1 0 1 1 0 0], Total Rewards: 989.838404498576, Epsilon: 0.5344229416520513\n",
      "Episode: 126, state: [0 1 1 0 1 1 0 1 1 1 1 1 1 0], Total Rewards: 1022.8070136066258, Epsilon: 0.531750826943791\n",
      "Episode: 127, state: [0 1 0 1 1 1 1 0 0 0 1 0 0 0], Total Rewards: 1100.780529811276, Epsilon: 0.5290920728090721\n",
      "Episode: 128, state: [0 1 0 1 1 0 1 0 0 0 0 1 0 1], Total Rewards: 1013.410256488639, Epsilon: 0.5264466124450268\n",
      "Episode: 129, state: [1 1 1 0 0 1 1 0 1 0 1 0 0 1], Total Rewards: 1064.8849960071502, Epsilon: 0.5238143793828016\n",
      "Episode: 130, state: [0 1 1 0 1 1 1 0 0 1 0 1 0 1], Total Rewards: 1040.4226293041181, Epsilon: 0.5211953074858876\n",
      "Episode: 131, state: [1 0 0 1 0 1 0 1 0 0 1 0 0 0], Total Rewards: 1041.0985689773488, Epsilon: 0.5185893309484582\n",
      "Episode: 132, state: [1 1 1 0 1 1 1 1 0 0 1 1 1 1], Total Rewards: 1085.443376837808, Epsilon: 0.5159963842937159\n",
      "Episode: 133, state: [1 0 0 0 1 0 0 1 0 1 1 0 1 0], Total Rewards: 1062.6615051273025, Epsilon: 0.5134164023722473\n",
      "Episode: 134, state: [1 0 0 0 0 1 1 0 1 0 0 0 1 1], Total Rewards: 1005.5803908415575, Epsilon: 0.510849320360386\n",
      "Episode: 135, state: [1 1 0 1 1 1 1 1 0 0 1 1 1 0], Total Rewards: 984.4562951613318, Epsilon: 0.5082950737585841\n",
      "Episode: 136, state: [0 0 0 1 1 0 0 1 0 0 1 1 1 1], Total Rewards: 1028.999718643457, Epsilon: 0.5057535983897912\n",
      "Episode: 137, state: [1 0 0 0 0 0 0 0 1 1 0 1 0 0], Total Rewards: 1139.821954967748, Epsilon: 0.5032248303978422\n",
      "Episode: 138, state: [1 1 1 1 1 0 0 1 0 1 1 0 1 0], Total Rewards: 1109.347983809117, Epsilon: 0.500708706245853\n",
      "Episode: 139, state: [0 1 1 0 0 1 1 1 1 1 1 1 0 1], Total Rewards: 1051.361727463353, Epsilon: 0.4982051627146237\n",
      "Episode: 140, state: [0 0 1 1 0 1 1 1 1 1 0 1 0 1], Total Rewards: 1084.7872295300942, Epsilon: 0.49571413690105054\n",
      "Episode: 141, state: [1 1 1 1 1 0 0 0 0 0 1 0 0 0], Total Rewards: 1093.9623144396423, Epsilon: 0.4932355662165453\n",
      "Episode: 142, state: [1 1 1 1 0 0 0 0 1 1 1 1 0 1], Total Rewards: 1054.5899522020932, Epsilon: 0.4907693883854626\n",
      "Episode: 143, state: [0 1 1 1 1 0 1 0 1 0 1 1 1 1], Total Rewards: 1078.5109042142267, Epsilon: 0.4883155414435353\n",
      "Episode: 144, state: [0 1 1 1 1 1 1 1 1 0 1 0 1 1], Total Rewards: 989.9205915494464, Epsilon: 0.4858739637363176\n",
      "Episode: 145, state: [1 0 1 1 0 1 0 1 1 1 0 1 0 1], Total Rewards: 1096.0946580409716, Epsilon: 0.483444593917636\n",
      "Episode: 146, state: [0 0 0 1 1 0 0 0 0 1 0 0 0 0], Total Rewards: 1071.9120204433718, Epsilon: 0.4810273709480478\n",
      "Episode: 147, state: [1 0 1 0 1 1 1 1 0 1 0 1 0 0], Total Rewards: 1152.7760610624152, Epsilon: 0.47862223409330756\n",
      "Episode: 148, state: [1 0 0 1 0 0 1 1 1 1 0 0 0 1], Total Rewards: 1130.2983003338295, Epsilon: 0.47622912292284103\n",
      "Episode: 149, state: [0 0 0 0 0 0 0 0 1 0 0 0 0 1], Total Rewards: 990.0552356101672, Epsilon: 0.4738479773082268\n",
      "Episode: 150, state: [0 1 0 1 1 0 1 0 0 1 1 0 1 0], Total Rewards: 1121.2114134461922, Epsilon: 0.47147873742168567\n",
      "Episode: 151, state: [0 0 1 1 1 0 1 0 0 1 1 0 0 0], Total Rewards: 1071.075661149787, Epsilon: 0.46912134373457726\n",
      "Episode: 152, state: [1 0 0 1 1 0 0 0 0 1 0 0 1 1], Total Rewards: 1084.7151650365522, Epsilon: 0.46677573701590436\n",
      "Episode: 153, state: [0 1 0 1 0 0 0 1 0 1 1 0 1 0], Total Rewards: 1141.6392078276854, Epsilon: 0.46444185833082485\n",
      "Episode: 154, state: [0 0 1 1 1 0 1 1 1 1 1 0 0 1], Total Rewards: 1042.4523385955376, Epsilon: 0.46211964903917074\n",
      "Episode: 155, state: [0 0 1 1 0 1 1 0 0 1 0 0 1 1], Total Rewards: 1110.7209750929094, Epsilon: 0.4598090507939749\n",
      "Episode: 156, state: [1 0 0 0 1 1 1 0 0 1 1 0 1 1], Total Rewards: 1068.8144475971515, Epsilon: 0.457510005540005\n",
      "Episode: 157, state: [0 1 0 1 1 1 1 1 0 0 1 1 1 1], Total Rewards: 1014.4500679169612, Epsilon: 0.45522245551230495\n",
      "Episode: 158, state: [1 0 0 0 1 0 0 1 1 1 1 1 0 0], Total Rewards: 1043.272914698896, Epsilon: 0.4529463432347434\n",
      "Episode: 159, state: [0 0 1 0 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1149.4812923352574, Epsilon: 0.4506816115185697\n",
      "Episode: 160, state: [1 1 1 1 1 1 1 1 0 1 0 0 0 1], Total Rewards: 1094.3455669945047, Epsilon: 0.4484282034609769\n",
      "Episode: 161, state: [0 1 0 1 1 0 0 0 0 0 0 0 1 1], Total Rewards: 1029.091345981947, Epsilon: 0.446186062443672\n",
      "Episode: 162, state: [1 1 0 1 0 1 0 0 0 1 0 0 0 1], Total Rewards: 1047.6122627131163, Epsilon: 0.4439551321314536\n",
      "Episode: 163, state: [0 1 0 0 0 0 1 0 1 0 1 0 1 1], Total Rewards: 1143.5261926612613, Epsilon: 0.4417353564707963\n",
      "Episode: 164, state: [0 1 0 0 0 1 1 0 1 0 0 0 0 0], Total Rewards: 970.2373823327257, Epsilon: 0.43952667968844233\n",
      "Episode: 165, state: [0 0 1 0 1 0 0 0 1 0 0 0 0 1], Total Rewards: 1045.0103455211859, Epsilon: 0.43732904629000013\n",
      "Episode: 166, state: [1 1 1 1 1 0 0 0 1 1 0 0 1 0], Total Rewards: 1138.6608857824608, Epsilon: 0.4351424010585501\n",
      "Episode: 167, state: [1 0 0 0 1 0 1 1 1 1 0 1 1 0], Total Rewards: 1034.8065808888261, Epsilon: 0.43296668905325736\n",
      "Episode: 168, state: [1 0 0 1 1 1 0 1 0 0 0 0 0 1], Total Rewards: 1126.5084596119239, Epsilon: 0.43080185560799106\n",
      "Episode: 169, state: [0 0 0 0 0 1 0 0 0 0 0 1 0 0], Total Rewards: 1101.7102178367627, Epsilon: 0.4286478463299511\n",
      "Episode: 170, state: [1 1 1 0 0 0 1 1 1 1 1 0 0 0], Total Rewards: 1111.7978159291526, Epsilon: 0.42650460709830135\n",
      "Episode: 171, state: [1 0 0 0 1 0 1 1 1 1 0 0 0 1], Total Rewards: 1143.1960021657806, Epsilon: 0.42437208406280985\n",
      "Episode: 172, state: [1 1 1 1 1 0 0 0 1 1 1 1 0 1], Total Rewards: 1066.9668939670735, Epsilon: 0.4222502236424958\n",
      "Episode: 173, state: [1 0 1 1 1 1 1 1 0 0 1 0 1 1], Total Rewards: 1095.7359912265997, Epsilon: 0.42013897252428334\n",
      "Episode: 174, state: [1 1 0 1 0 1 1 0 0 0 0 0 1 0], Total Rewards: 1130.4409488700671, Epsilon: 0.4180382776616619\n",
      "Episode: 175, state: [1 0 0 0 0 0 0 1 1 1 1 1 0 1], Total Rewards: 1101.055506470153, Epsilon: 0.4159480862733536\n",
      "Episode: 176, state: [0 0 0 0 1 1 1 1 0 0 1 0 0 0], Total Rewards: 1046.3212985828538, Epsilon: 0.41386834584198684\n",
      "Episode: 177, state: [1 0 1 0 1 1 1 0 0 1 1 0 1 0], Total Rewards: 1183.8249140240882, Epsilon: 0.4117990041127769\n",
      "Episode: 178, state: [0 1 1 0 1 0 1 0 0 1 0 0 1 0], Total Rewards: 1048.5251442756016, Epsilon: 0.40974000909221303\n",
      "Episode: 179, state: [0 1 1 1 1 0 1 1 1 0 0 0 0 1], Total Rewards: 1075.3380958112825, Epsilon: 0.40769130904675194\n",
      "Episode: 180, state: [1 0 1 1 1 1 0 0 0 1 0 1 1 1], Total Rewards: 1110.9092110968768, Epsilon: 0.40565285250151817\n",
      "Episode: 181, state: [1 0 1 0 1 0 0 1 1 0 1 0 1 1], Total Rewards: 1043.2196427794963, Epsilon: 0.4036245882390106\n",
      "Episode: 182, state: [0 1 0 1 1 1 0 1 1 0 0 0 0 1], Total Rewards: 1085.3286733599014, Epsilon: 0.4016064652978155\n",
      "Episode: 183, state: [0 0 1 1 1 0 1 0 1 1 1 1 1 0], Total Rewards: 1043.9179636661077, Epsilon: 0.3995984329713264\n",
      "Episode: 184, state: [0 0 0 0 0 1 1 0 0 1 1 0 0 1], Total Rewards: 1136.3308480525466, Epsilon: 0.3976004408064698\n",
      "Episode: 185, state: [1 1 0 0 1 1 1 0 0 0 1 1 0 0], Total Rewards: 1091.4117680194565, Epsilon: 0.39561243860243744\n",
      "Episode: 186, state: [1 0 0 1 1 1 1 1 1 1 1 1 1 0], Total Rewards: 1144.7855252764016, Epsilon: 0.3936343764094253\n",
      "Episode: 187, state: [1 0 1 1 0 1 0 0 0 1 1 1 1 1], Total Rewards: 1064.6916906235047, Epsilon: 0.39166620452737816\n",
      "Episode: 188, state: [1 0 1 0 0 1 1 0 1 1 1 0 0 0], Total Rewards: 1042.6098897687034, Epsilon: 0.3897078735047413\n",
      "Episode: 189, state: [0 0 0 1 0 0 1 0 1 0 0 1 0 0], Total Rewards: 1100.7596676215915, Epsilon: 0.3877593341372176\n",
      "Episode: 190, state: [0 0 0 1 1 1 1 0 0 0 0 0 1 1], Total Rewards: 1064.854187157232, Epsilon: 0.3858205374665315\n",
      "Episode: 191, state: [1 1 1 1 1 1 1 1 0 1 1 1 1 1], Total Rewards: 1087.252524813812, Epsilon: 0.38389143477919885\n",
      "Episode: 192, state: [1 0 0 1 1 0 0 1 1 0 1 0 0 0], Total Rewards: 1113.7001947454742, Epsilon: 0.3819719776053028\n",
      "Episode: 193, state: [1 1 1 1 1 1 1 1 0 0 1 0 1 1], Total Rewards: 1097.8879028206966, Epsilon: 0.3800621177172763\n",
      "Episode: 194, state: [1 1 1 1 1 0 1 0 1 1 0 0 0 1], Total Rewards: 1141.534246594252, Epsilon: 0.37816180712868996\n",
      "Episode: 195, state: [1 0 1 1 0 0 1 0 1 1 1 0 0 0], Total Rewards: 1095.2023779159033, Epsilon: 0.37627099809304654\n",
      "Episode: 196, state: [1 1 1 0 0 0 1 1 0 0 0 0 1 1], Total Rewards: 1054.0188492529155, Epsilon: 0.3743896431025813\n",
      "Episode: 197, state: [1 0 0 1 0 1 0 1 0 0 1 0 0 1], Total Rewards: 1247.0093310238972, Epsilon: 0.37251769488706843\n",
      "Episode: 198, state: [0 1 1 1 1 1 1 0 1 1 1 0 1 0], Total Rewards: 1093.3564944501973, Epsilon: 0.3706551064126331\n",
      "Episode: 199, state: [1 1 1 1 1 1 0 1 0 1 1 0 1 1], Total Rewards: 1067.3392008922144, Epsilon: 0.36880183088056995\n",
      "Episode: 200, state: [0 1 1 0 1 0 1 0 1 1 1 0 1 1], Total Rewards: 1066.7761768285736, Epsilon: 0.3669578217261671\n",
      "Episode: 201, state: [1 0 1 1 0 1 1 0 1 0 1 1 0 0], Total Rewards: 1175.7510440877197, Epsilon: 0.36512303261753626\n",
      "Episode: 202, state: [0 1 1 1 1 0 1 1 1 1 1 1 0 1], Total Rewards: 1129.174567921658, Epsilon: 0.3632974174544486\n",
      "Episode: 203, state: [0 1 1 1 1 1 1 1 1 0 0 1 1 1], Total Rewards: 1178.8514336060505, Epsilon: 0.3614809303671764\n",
      "Episode: 204, state: [1 0 0 1 1 0 1 1 0 1 0 1 1 1], Total Rewards: 1189.406521146598, Epsilon: 0.3596735257153405\n",
      "Episode: 205, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 0], Total Rewards: 1115.7757380869161, Epsilon: 0.3578751580867638\n",
      "Episode: 206, state: [0 0 1 1 1 1 0 0 0 1 0 1 1 1], Total Rewards: 1033.164873468231, Epsilon: 0.35608578229633\n",
      "Episode: 207, state: [1 1 1 0 1 0 1 0 1 0 1 0 1 1], Total Rewards: 1210.7851366772952, Epsilon: 0.3543053533848483\n",
      "Episode: 208, state: [0 1 0 0 0 0 1 1 0 0 0 0 0 1], Total Rewards: 1090.1528992670112, Epsilon: 0.35253382661792404\n",
      "Episode: 209, state: [1 1 0 1 1 1 1 1 1 1 1 1 0 0], Total Rewards: 1122.5622035987537, Epsilon: 0.3507711574848344\n",
      "Episode: 210, state: [0 1 1 0 1 1 0 1 1 1 1 1 0 0], Total Rewards: 1119.6021580348943, Epsilon: 0.34901730169741024\n",
      "Episode: 211, state: [0 0 0 1 1 0 0 1 1 1 1 0 1 0], Total Rewards: 1189.9867013353507, Epsilon: 0.3472722151889232\n",
      "Episode: 212, state: [0 0 0 1 0 0 1 0 0 0 1 1 1 0], Total Rewards: 1087.2612898201173, Epsilon: 0.3455358541129786\n",
      "Episode: 213, state: [0 0 1 0 1 1 1 1 1 1 0 0 0 1], Total Rewards: 1078.0098672502104, Epsilon: 0.3438081748424137\n",
      "Episode: 214, state: [1 1 1 1 1 1 1 1 1 1 0 1 1 0], Total Rewards: 1110.6182781017503, Epsilon: 0.3420891339682016\n",
      "Episode: 215, state: [0 0 1 1 1 1 1 1 1 1 0 1 1 0], Total Rewards: 1096.022107357796, Epsilon: 0.3403786882983606\n",
      "Episode: 216, state: [1 1 1 1 1 0 1 1 0 1 1 1 1 1], Total Rewards: 1189.3934034321062, Epsilon: 0.3386767948568688\n",
      "Episode: 217, state: [1 1 0 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1152.1412019888105, Epsilon: 0.33698341088258443\n",
      "Episode: 218, state: [1 1 1 1 1 1 0 1 0 1 1 1 1 1], Total Rewards: 1127.0664301609602, Epsilon: 0.3352984938281715\n",
      "Episode: 219, state: [1 0 1 1 0 0 1 1 1 0 0 0 0 0], Total Rewards: 1182.455161795149, Epsilon: 0.33362200135903064\n",
      "Episode: 220, state: [0 0 1 0 0 1 1 0 0 1 1 0 1 1], Total Rewards: 1171.5985390900826, Epsilon: 0.33195389135223546\n",
      "Episode: 221, state: [1 0 1 1 0 1 0 1 1 1 1 1 1 1], Total Rewards: 1096.316293593229, Epsilon: 0.3302941218954743\n",
      "Episode: 222, state: [0 0 1 0 1 1 1 1 0 0 1 1 1 1], Total Rewards: 1070.5318930225942, Epsilon: 0.32864265128599696\n",
      "Episode: 223, state: [1 1 0 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1094.625023091235, Epsilon: 0.326999438029567\n",
      "Episode: 224, state: [1 1 1 1 1 1 1 1 0 1 1 1 1 1], Total Rewards: 1111.4890863101232, Epsilon: 0.3253644408394192\n",
      "Episode: 225, state: [0 1 1 0 1 0 1 1 1 1 0 1 0 0], Total Rewards: 1171.782940970948, Epsilon: 0.3237376186352221\n",
      "Episode: 226, state: [1 1 1 1 0 1 0 1 1 1 0 1 1 1], Total Rewards: 1182.8020395239457, Epsilon: 0.322118930542046\n",
      "Episode: 227, state: [1 1 1 1 1 1 0 1 1 0 0 1 1 0], Total Rewards: 1191.193543240098, Epsilon: 0.32050833588933575\n",
      "Episode: 228, state: [1 1 1 1 0 1 0 0 0 0 0 1 1 0], Total Rewards: 1104.7836362954904, Epsilon: 0.31890579420988907\n",
      "Episode: 229, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1165.2453619849682, Epsilon: 0.3173112652388396\n",
      "Episode: 230, state: [0 1 0 0 1 0 0 0 0 0 1 0 0 0], Total Rewards: 1088.4547661472102, Epsilon: 0.3157247089126454\n",
      "Episode: 231, state: [0 0 1 1 0 0 0 1 0 0 0 1 0 1], Total Rewards: 1112.4754018273625, Epsilon: 0.3141460853680822\n",
      "Episode: 232, state: [1 0 0 0 0 1 1 1 0 1 1 0 1 1], Total Rewards: 1119.5503238396518, Epsilon: 0.3125753549412418\n",
      "Episode: 233, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1122.7382758672086, Epsilon: 0.31101247816653554\n",
      "Episode: 234, state: [0 0 0 1 0 1 0 0 1 0 0 1 0 1], Total Rewards: 1203.6787769115253, Epsilon: 0.30945741577570285\n",
      "Episode: 235, state: [1 1 1 1 1 1 1 1 1 1 1 0 1 1], Total Rewards: 1121.5401772751975, Epsilon: 0.3079101286968243\n",
      "Episode: 236, state: [0 1 1 1 0 1 0 0 1 1 1 0 1 0], Total Rewards: 1175.0011865856013, Epsilon: 0.3063705780533402\n",
      "Episode: 237, state: [0 1 1 1 1 1 1 1 1 0 1 1 0 1], Total Rewards: 1152.6838074318841, Epsilon: 0.30483872516307353\n",
      "Episode: 238, state: [1 0 0 0 0 1 0 1 0 1 1 1 1 1], Total Rewards: 1143.495661694011, Epsilon: 0.3033145315372582\n",
      "Episode: 239, state: [0 1 1 1 0 1 0 0 1 1 0 0 0 0], Total Rewards: 1081.881086506075, Epsilon: 0.3017979588795719\n",
      "Episode: 240, state: [1 1 1 0 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1092.9460145254266, Epsilon: 0.30028896908517405\n",
      "Episode: 241, state: [1 0 0 1 1 1 0 0 1 0 1 1 1 1], Total Rewards: 1263.426909784257, Epsilon: 0.2987875242397482\n",
      "Episode: 242, state: [1 1 0 0 1 0 1 1 0 1 1 1 1 0], Total Rewards: 1196.9985773636895, Epsilon: 0.29729358661854943\n",
      "Episode: 243, state: [1 1 1 1 1 0 1 0 1 0 1 1 1 1], Total Rewards: 1222.030988890229, Epsilon: 0.29580711868545667\n",
      "Episode: 244, state: [1 0 1 1 0 1 0 1 1 1 0 0 1 0], Total Rewards: 1119.227039876774, Epsilon: 0.2943280830920294\n",
      "Episode: 245, state: [1 1 1 1 0 0 0 0 1 1 1 0 1 1], Total Rewards: 1185.0701897030465, Epsilon: 0.29285644267656924\n",
      "Episode: 246, state: [1 0 1 1 1 1 1 0 0 1 0 1 1 0], Total Rewards: 1192.8246258023253, Epsilon: 0.2913921604631864\n",
      "Episode: 247, state: [0 1 1 1 0 0 0 1 1 1 1 0 0 0], Total Rewards: 1082.4970610118091, Epsilon: 0.28993519966087045\n",
      "Episode: 248, state: [0 1 0 0 1 0 1 1 1 1 0 1 1 0], Total Rewards: 1184.4833236768134, Epsilon: 0.2884855236625661\n",
      "Episode: 249, state: [1 1 0 0 0 1 1 1 1 1 1 1 0 1], Total Rewards: 1199.7909384206253, Epsilon: 0.28704309604425327\n",
      "Episode: 250, state: [0 0 1 0 1 0 0 1 0 0 1 1 1 1], Total Rewards: 1144.4009923357053, Epsilon: 0.285607880564032\n",
      "Episode: 251, state: [1 1 1 1 1 1 0 1 1 1 1 1 1 1], Total Rewards: 1120.1038697896208, Epsilon: 0.28417984116121187\n",
      "Episode: 252, state: [1 1 1 1 0 0 1 1 1 1 1 1 0 1], Total Rewards: 1147.1424623388775, Epsilon: 0.2827589419554058\n",
      "Episode: 253, state: [1 1 0 1 0 1 1 1 0 1 1 1 1 0], Total Rewards: 1173.441626492048, Epsilon: 0.28134514724562876\n",
      "Episode: 254, state: [1 1 0 1 1 1 1 1 1 1 0 1 1 0], Total Rewards: 1148.2175162569222, Epsilon: 0.2799384215094006\n",
      "Episode: 255, state: [1 1 0 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1141.1968994834021, Epsilon: 0.27853872940185365\n",
      "Episode: 256, state: [0 0 1 1 1 1 1 1 1 1 1 1 0 1], Total Rewards: 1083.6171256487373, Epsilon: 0.27714603575484437\n",
      "Episode: 257, state: [1 1 1 1 1 1 1 1 0 1 1 1 0 1], Total Rewards: 1295.5034629605316, Epsilon: 0.2757603055760701\n",
      "Episode: 258, state: [1 0 1 1 1 1 1 0 1 0 0 1 0 1], Total Rewards: 1149.0589258557686, Epsilon: 0.2743815040481898\n",
      "Episode: 259, state: [1 1 0 1 1 0 0 0 0 1 1 0 1 1], Total Rewards: 1189.0358424506205, Epsilon: 0.2730095965279488\n",
      "Episode: 260, state: [0 1 1 1 0 0 1 1 0 0 1 0 0 1], Total Rewards: 1216.8171167627318, Epsilon: 0.27164454854530906\n",
      "Episode: 261, state: [0 1 0 1 0 0 1 1 1 0 0 0 1 0], Total Rewards: 1184.327234579078, Epsilon: 0.2702863258025825\n",
      "Episode: 262, state: [0 1 0 1 0 1 0 1 1 1 0 0 1 1], Total Rewards: 1208.3756865224407, Epsilon: 0.2689348941735696\n",
      "Episode: 263, state: [1 0 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1231.7956861422088, Epsilon: 0.26759021970270175\n",
      "Episode: 264, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1189.1636445651006, Epsilon: 0.2662522686041882\n",
      "Episode: 265, state: [1 0 1 1 1 1 1 0 0 1 1 0 1 0], Total Rewards: 1153.4993123113816, Epsilon: 0.2649210072611673\n",
      "Episode: 266, state: [1 1 0 1 0 0 0 1 0 0 1 1 0 1], Total Rewards: 1293.0806303484887, Epsilon: 0.26359640222486147\n",
      "Episode: 267, state: [1 1 0 1 1 1 0 1 0 1 0 1 1 1], Total Rewards: 1190.4942145097805, Epsilon: 0.26227842021373715\n",
      "Episode: 268, state: [1 0 0 1 1 1 0 1 1 1 1 1 1 1], Total Rewards: 1197.7154135712335, Epsilon: 0.2609670281126685\n",
      "Episode: 269, state: [1 1 0 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1154.0010689537812, Epsilon: 0.25966219297210513\n",
      "Episode: 270, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1142.696193544938, Epsilon: 0.2583638820072446\n",
      "Episode: 271, state: [1 0 1 0 1 1 0 0 0 0 0 0 0 0], Total Rewards: 1186.8985776089391, Epsilon: 0.2570720625972084\n",
      "Episode: 272, state: [1 0 0 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1192.3196821260842, Epsilon: 0.25578670228422234\n",
      "Episode: 273, state: [1 1 1 1 1 1 1 0 1 1 1 1 1 1], Total Rewards: 1240.5986348715194, Epsilon: 0.25450776877280124\n",
      "Episode: 274, state: [1 1 0 0 1 1 1 1 0 0 1 1 1 1], Total Rewards: 1311.7312854235593, Epsilon: 0.2532352299289372\n",
      "Episode: 275, state: [1 1 1 1 1 1 1 1 1 1 0 1 1 1], Total Rewards: 1262.2700141217576, Epsilon: 0.2519690537792925\n",
      "Episode: 276, state: [1 1 1 1 1 1 1 1 1 1 1 1 0 0], Total Rewards: 1223.3200918098612, Epsilon: 0.2507092085103961\n",
      "Episode: 277, state: [0 0 0 0 0 1 0 0 0 1 1 1 0 1], Total Rewards: 1176.224724454322, Epsilon: 0.2494556624678441\n",
      "Episode: 278, state: [1 0 0 1 0 1 0 1 0 1 0 1 0 0], Total Rewards: 1174.6149070207061, Epsilon: 0.24820838415550486\n",
      "Episode: 279, state: [0 0 0 1 1 1 0 1 1 0 0 0 0 0], Total Rewards: 1189.582749189808, Epsilon: 0.24696734223472733\n",
      "Episode: 280, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1151.3808897231781, Epsilon: 0.2457325055235537\n",
      "Episode: 281, state: [1 0 1 1 1 0 0 0 1 1 0 1 0 1], Total Rewards: 1147.5332676946293, Epsilon: 0.24450384299593592\n",
      "Episode: 282, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1187.3122066571402, Epsilon: 0.24328132378095624\n",
      "Episode: 283, state: [0 1 0 1 1 1 1 0 1 1 0 1 1 0], Total Rewards: 1195.902496153383, Epsilon: 0.24206491716205145\n",
      "Episode: 284, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1167.1430687387954, Epsilon: 0.2408545925762412\n",
      "Episode: 285, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1158.6290751872484, Epsilon: 0.23965031961336\n",
      "Episode: 286, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1200.495546019462, Epsilon: 0.2384520680152932\n",
      "Episode: 287, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1150.9295574609218, Epsilon: 0.23725980767521673\n",
      "Episode: 288, state: [0 1 1 1 1 1 0 1 1 1 1 0 1 0], Total Rewards: 1289.1834844766072, Epsilon: 0.23607350863684065\n",
      "Episode: 289, state: [0 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1192.8848972924652, Epsilon: 0.23489314109365644\n",
      "Episode: 290, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1139.5859337825925, Epsilon: 0.23371867538818816\n",
      "Episode: 291, state: [1 1 1 1 1 1 1 0 1 1 1 0 1 1], Total Rewards: 1190.3129421408826, Epsilon: 0.23255008201124722\n",
      "Episode: 292, state: [1 1 0 0 1 1 0 1 1 1 0 1 1 0], Total Rewards: 1259.7857576019644, Epsilon: 0.231387331601191\n",
      "Episode: 293, state: [0 1 1 1 1 1 1 0 1 0 1 1 1 1], Total Rewards: 1204.3577152318467, Epsilon: 0.23023039494318503\n",
      "Episode: 294, state: [1 1 0 1 1 1 1 1 1 1 1 0 1 1], Total Rewards: 1215.1942610711592, Epsilon: 0.2290792429684691\n",
      "Episode: 295, state: [1 1 0 0 1 1 1 1 1 1 1 1 1 0], Total Rewards: 1251.9147153289355, Epsilon: 0.22793384675362674\n",
      "Episode: 296, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1213.4224551182433, Epsilon: 0.22679417751985861\n",
      "Episode: 297, state: [1 1 0 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1206.5547822496226, Epsilon: 0.22566020663225933\n",
      "Episode: 298, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1215.6669656785068, Epsilon: 0.22453190559909803\n",
      "Episode: 299, state: [1 1 1 1 1 1 1 1 1 1 0 1 0 1], Total Rewards: 1169.8315333966195, Epsilon: 0.22340924607110255\n",
      "Episode: 300, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1228.190905535704, Epsilon: 0.22229219984074702\n",
      "Episode: 301, state: [1 1 0 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1224.7147697168969, Epsilon: 0.2211807388415433\n",
      "Episode: 302, state: [0 1 0 1 1 0 1 1 1 0 0 0 1 1], Total Rewards: 1285.3263754027557, Epsilon: 0.22007483514733558\n",
      "Episode: 303, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1223.6005902325244, Epsilon: 0.2189744609715989\n",
      "Episode: 304, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1197.8105429093719, Epsilon: 0.2178795886667409\n",
      "Episode: 305, state: [1 1 1 1 1 1 1 1 1 1 1 1 1 1], Total Rewards: 1149.7228957025743, Epsilon: 0.2167901907234072\n",
      "Episode: 306, state: [1 1 1 0 0 0 1 0 1 0 0 1 1 0], Total Rewards: 1269.9579159022937, Epsilon: 0.21570623976979014\n",
      "Episode: 307, state: [1 1 0 1 1 0 1 1 1 1 1 1 1 1], Total Rewards: 1272.4560419331078, Epsilon: 0.21462770857094118\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 99\u001b[0m\n\u001b[1;32m     96\u001b[0m q_values \u001b[38;5;241m=\u001b[39m model(torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m     97\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(q_values[actions[i]], target)\n\u001b[0;32m---> 99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    100\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    101\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.11/site-packages/torch/optim/optimizer.py:267\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m foreach:\n\u001b[1;32m    266\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: defaultdict(\u001b[38;5;28mlist\u001b[39m))\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_profile_name):\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/torch/lib/python3.11/site-packages/torch/autograd/profiler.py:485\u001b[0m, in \u001b[0;36mrecord_function.__init__\u001b[0;34m(self, name, args)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks_on_exit: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Stores underlying RecordFunction as a tensor. TODO: move to custom\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# class (https://github.com/pytorch/pytorch/issues/35026).\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Main Function\n",
    "\n",
    "# Read input dataset\n",
    "run_folder = '/Users/maitreyeesharma/WORKSPACE/PostDoc/Chemistry/SPIRAL/codes/RL/ReLMM/scripts/'\n",
    "\n",
    "# Reading the input json file with dataset filename and path information\n",
    "with open(run_folder+'inputs.json', \"r\") as f:\n",
    "    input_dict = json.load(f)\n",
    "\n",
    "input_type = input_dict['InputType']\n",
    "input_path = input_dict['InputPath']\n",
    "input_file = input_dict['InputFile']\n",
    "output_dir = input_dict['OutputDirectory']\n",
    "\n",
    "# Create a new output directory if it does not exist\n",
    "isExist = os.path.exists(output_dir)\n",
    "if not isExist:\n",
    "    os.makedirs(output_dir)\n",
    "    print(\"The new directory is created!\", output_dir)\n",
    "\n",
    "input_data = inputs(input_type=input_type,\n",
    "                           input_path=input_path,\n",
    "                           input_file=input_file)\n",
    "\n",
    "X_data, Y_data, descriptors = input_data.read_inputs()\n",
    "X_stand_all, X_stand_df_all, scalerX = utilsd.standardize_data(X_data)\n",
    "Y_stand_all, Y_stand_df_all, scalerY = utilsd.standardize_data(pd.DataFrame({'target':Y_data[:,0]}))\n",
    "# X_stand_df, X_test_df, Y_stand_df, Y_test_df = train_test_split(X_stand_df_all, Y_stand_df_all, test_size=0.1, random_state=40)\n",
    "# X_stand, X_test, Y_stand, Y_test = train_test_split(X_stand_all, Y_stand_all, test_size=0.1, random_state=40)\n",
    "X_stand_df = X_stand_df_all\n",
    "Y_stand_df = Y_stand_df_all\n",
    "\n",
    "# Dataset parameters\n",
    "total_num_features = len(descriptors)\n",
    "\n",
    "# Environment parameters\n",
    "state_size = total_num_features  # Size of the state space\n",
    "N_agents = total_num_features # Number of agents\n",
    "action_size = 2  # Number of possible actions\n",
    "N_steps = 100 # Number of steps to take per episode\n",
    "predictor_model = predictor_models()\n",
    "\n",
    "# Hyperparameters\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_decay = 0.995  # Decay rate of exploration\n",
    "gamma = 0.95  # Discount factor\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize environment and Q-networks for each agent\n",
    "env = Environment(state_size,action_size,N_agents,N_steps)\n",
    "agent_model = {}\n",
    "agent_optimizer = {}\n",
    "agent_qvalue = {}\n",
    "\n",
    "for i_agent in range(N_agents):\n",
    "    model_name = 'agent'+str(i_agent)+'_model'\n",
    "    optimizer_name = 'agent'+str(i_agent)+'_optimizer'\n",
    "    agent_model[model_name] = QNetwork(env.state_size, env.action_size)\n",
    "    agent_optimizer[optimizer_name] = optim.Adam(agent_model[model_name].parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "total_episodes = 1000\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    state = env.reset()\n",
    "    total_rewards = np.zeros(N_agents) # Total rewards for agents\n",
    "    \n",
    "    while True:\n",
    "        # Agents choose actions using epsilon-greedy policy\n",
    "        if np.random.rand() <= epsilon:\n",
    "            actions = np.random.randint(2, size=(N_agents,))  # Random actions\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                actions_list = []\n",
    "                for i_agent in range(N_agents):\n",
    "                    model_name = 'agent'+str(i_agent)+'_model'\n",
    "                    q_values = agent_model[model_name](torch.tensor(state, dtype=torch.float32))\n",
    "                    actions_list.append(torch.argmax(q_values).item())\n",
    "                actions = np.array(actions_list)\n",
    "                \n",
    "        if all(action == 0 for action in actions):\n",
    "            non_zero_action = np.random.randint(N_agents)\n",
    "            actions[non_zero_action] = 1\n",
    "        \n",
    "        # Take actions and observe next states, rewards, done\n",
    "        next_state, done = env.step(actions)\n",
    "        rewards = env.get_rewards(predictor_model,X_stand_df,Y_stand_df)\n",
    "\n",
    "        # Update Q-values for each agent\n",
    "        for i, (model, optimizer, reward) in enumerate(zip(agent_model.values(),\n",
    "                                                           agent_optimizer.values(),\n",
    "                                                           rewards)):\n",
    "            q_values_next = model(torch.tensor(next_state, dtype=torch.float32))\n",
    "            target = reward + gamma * torch.max(q_values_next)\n",
    "\n",
    "            q_values = model(torch.tensor(state, dtype=torch.float32))\n",
    "            loss = nn.functional.mse_loss(q_values[actions[i]], target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_rewards[i] += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > 0.01:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Print episode results\n",
    "    if (episode + 1) % 1 == 0:\n",
    "        print(f\"Episode: {episode + 1}, state: {state}, Total Rewards: {total_rewards[0]}, Epsilon: {epsilon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad0ea0-4d70-41a8-9bb6-40dc60b8b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save QNetwork Models\n",
    "for i_agent in range(N_agents):\n",
    "    model_name = 'agent'+str(i_agent)+'_model'\n",
    "    saveModel_filename = output_dir+model_name+'.pt'\n",
    "    torch.save(agent_model[model_name].state_dict(), saveModel_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a2339-18fd-49f3-b6a6-e2489f959719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26530d9-7a41-4ac5-805b-92abbcd373d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the trained agents\n",
    "state = env.reset()\n",
    "total_rewards = np.zeros(N_agents)\n",
    "\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        actions_list = []\n",
    "        for i_agent in range(N_agents):\n",
    "            model_name = 'agent'+str(i_agent)+'_model'\n",
    "            q_values = agent_model[model_name](torch.tensor(state, dtype=torch.float32))\n",
    "            actions_list.append(torch.argmax(q_values).item())        \n",
    "        actions = np.array(actions_list)\n",
    "\n",
    "    next_state, done = env.step(actions)\n",
    "    rewards, feature_importance_dict_rl, mse_rl = env.get_rewards_test(predictor_model,X_stand_df,Y_stand_df)    \n",
    "    total_rewards += rewards  \n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "importance_df_rl = pd.DataFrame.from_dict(data=feature_importance_dict_rl, orient='index')\n",
    "importance_df_rl.to_csv(output_dir+'rl.csv')\n",
    "mse_df_rl = pd.DataFrame({'MSE_RL':[mse_rl]})   \n",
    "mse_df_rl.to_csv(output_dir+'rl_mse.csv')\n",
    "print(f\"Test Total Rewards: {total_rewards}, state: {state}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a409e-b8b5-4077-9398-8d0a766c5205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the trained agents\n",
    "state = env.reset()\n",
    "total_rewards = np.zeros(N_agents)\n",
    "\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        actions_list = []\n",
    "        for i_agent in range(N_agents):\n",
    "            model_name = 'agent'+str(i_agent)+'_model'\n",
    "            q_values = agent_model[model_name](torch.tensor(state, dtype=torch.float32))\n",
    "            actions_list.append(torch.argmax(q_values).item())        \n",
    "        actions = np.array(actions_list)\n",
    "\n",
    "    next_state, done = env.step(actions)\n",
    "    rewards, feature_importance_dict_rl, mse_rl = env.get_rewards_test(predictor_model,X_test_df,Y_test_df)    \n",
    "    total_rewards += rewards  \n",
    "    state = next_state\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "importance_df_rl = pd.DataFrame.from_dict(data=feature_importance_dict_rl, orient='index')\n",
    "importance_df_rl.to_csv(output_dir+'rl_test.csv')\n",
    "mse_df_rl = pd.DataFrame({'MSE_RL':[mse_rl]})   \n",
    "mse_df_rl.to_csv(output_dir+'rl_mse_test.csv')\n",
    "print(f\"Test Total Rewards: {total_rewards}, state: {state}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c443e58-7e6d-4971-ac2c-9282d849fd63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
